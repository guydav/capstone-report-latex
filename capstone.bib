@article{Abu-El-Haija2016,
abstract = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\~{}}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
archivePrefix = {arXiv},
arxivId = {1609.08675},
author = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
eprint = {1609.08675},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Benchmark.pdf:pdf},
month = {sep},
title = {{YouTube-8M: A Large-Scale Video Classification Benchmark}},
url = {http://arxiv.org/abs/1609.08675},
year = {2016}
}
@techreport{Agrawal2016,
abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance.},
archivePrefix = {arXiv},
arxivId = {1505.00468v6},
author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C Lawrence and Batra, Dhruv and Parikh, Devi},
eprint = {1505.00468v6},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Agrawal et al. - 2016 - VQA Visual Question Answering.pdf:pdf},
title = {{VQA: Visual Question Answering}},
url = {www.visualqa.org},
year = {2016}
}
@inproceedings{Al-Shedivat2018,
abstract = {The ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.},
archivePrefix = {arXiv},
arxivId = {1710.03641v2},
author = {Al-Shedivat, Maruan and Bansal, Trapit and Amherst, Umass and Burda, Yura and Ilya, Openai and Openai, Sutskever and Openai, Igor Mordatch and Abbeel, Pieter},
booktitle = {ICLR},
eprint = {1710.03641v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Al-Shedivat et al. - 2018 - Continuous Adaptation via Meta-Learning in Nonstationary and Copetitive Environments.pdf:pdf},
title = {{Continuous Adaptation via Meta-Learning in Nonstationary and Copetitive Environments}},
url = {https://goo.gl/tboqaN.},
year = {2018}
}
@inproceedings{Aljundi2018,
abstract = {Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network , MAS accumulates an importance measure for each parameter of the network , based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule, which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting {\textless}subject, predicate, object{\textgreater}triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.},
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {ECCV},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forget.pdf:pdf},
title = {{Memory Aware Synapses: Learning what (not) to forget}},
url = {http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/papers/Rahaf{\_}Aljundi{\_}Memory{\_}Aware{\_}Synapses{\_}ECCV{\_}2018{\_}paper.pdf},
year = {2018}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
eprint = {1606.06565},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
month = {jun},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@techreport{Andreas2016,
abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neu-ral networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question Answering.pdf:pdf},
title = {{Learning to Compose Neural Networks for Question Answering}},
url = {http://github.com/},
year = {2016}
}
@techreport{Andreas2015,
abstract = {Visual question answering is fundamentally composi-tional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the com-positional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks , which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic sub-structures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering , achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.02799v4},
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
eprint = {arXiv:1511.02799v4},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Andreas et al. - 2015 - Deep Compositional Question Answering with Neural Module Networks.pdf:pdf},
title = {{Deep Compositional Question Answering with Neural Module Networks}},
url = {https://arxiv.org/pdf/1511.02799.pdf},
year = {2015}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
eprint = {1606.04474},
month = {jun},
title = {{Learning to learn by gradient descent by gradient descent}},
url = {http://arxiv.org/abs/1606.04474},
year = {2016}
}
@inproceedings{Antol2015,
abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions , and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
booktitle = {ICCV},
file = {::},
pages = {2425--2433},
title = {{VQA: Visual Question Answering}},
url = {www.visualqa.org},
year = {2015}
}
@article{Bahrick1993,
abstract = {In a 9-year longitudinal investigation, 4 subjects learned and relearned 300 English-foreign language word pairs. Either 13 or 26 relearning sessions were administered at intervals of 14, 28, or 56 days. Retention was tested for 1.2.3. or 5 years after training terminated. The longer intersession intervals slowed down acquisition slightly, but this disadvantage during training was offset by substantially higher retention. Thirteen retraining sessions spaced at 56 days yielded retention comparable to 26 sessions spaced at 14 days. The retention benefit due to additional sessions was independent of the benefit due to spacing, and both variables facilitated retention of words regardless of difficulty level and of the consistency of retrieval during training. The benefits of spaced retrieval practice to long-term maintenance of access to academic knowledge areas are discussed.},
author = {Bahrick, Harry P. and Bahrick, Lorraine E. and Bahrick, Audrey S. and Bahrick, Phyllis E.},
doi = {10.1111/j.1467-9280.1993.tb00571.x},
issn = {0956-7976},
journal = {Psychological Science},
month = {sep},
number = {5},
pages = {316--321},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Maintenance of Foreign Language Vocabulary and the Spacing Effect}},
url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.1993.tb00571.x},
volume = {4},
year = {1993}
}
@book{Bailey2008,
abstract = {This book should be on the shelf of every practising statistician who designs experiments. Good design considers units and treatments first, and then allocates treatments to units. It does not choose from a menu of named designs. This approach requires a notation for units that does not depend on the treatments applied. Most structure on the set of observational units, or on the set of treatments, can be defined by factors. This book develops a coherent framework for thinking about factors and their relationships, including the use of Hasse diagrams. These are used to elucidate structure, calculate degrees of freedom and allocate treatment subspaces to appropriate strata. Based on a one-term course the author has taught since 1989, the book is ideal for advanced undergraduate and beginning graduate courses. Examples, exercises and discussion questions are drawn from a wide range of real applications: from drug development, to agriculture, to manufacturing.},
author = {Bailey, R.},
isbn = {9780511611483},
pages = {330},
title = {{Design of comparative experiments}},
year = {2008}
}
@inproceedings{Bengio1991,
abstract = {The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.},
author = {Bengio, Y. and Bengio, S. and Cloutier, J.},
booktitle = {Seattle International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.1991.155621},
isbn = {0-7803-0164-1},
pages = {969},
publisher = {IEEE},
title = {{Learning a synaptic learning rule}},
url = {http://ieeexplore.ieee.org/document/155621/},
volume = {ii},
year = {1991}
}
@article{Benyahia2019,
abstract = {We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.},
archivePrefix = {arXiv},
arxivId = {1902.08232},
author = {Benyahia, Yassine and Yu, Kaicheng and Bennani-Smires, Kamil and Jaggi, Martin and Davison, Anthony and Salzmann, Mathieu and Musat, Claudiu},
eprint = {1902.08232},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Benyahia et al. - 2019 - Overcoming Multi-Model Forgetting.pdf:pdf},
month = {feb},
title = {{Overcoming Multi-Model Forgetting}},
url = {http://arxiv.org/abs/1902.08232},
year = {2019}
}
@misc{BlenderOnlineCommunity2018,
author = {{Blender Online Community}},
publisher = {Blender Foundation},
title = {{Blender - a 3D modelling and rendering package}},
url = {https://www.blender.org/},
year = {2018}
}
@article{Bojarski2016,
abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
archivePrefix = {arXiv},
arxivId = {1604.07316},
author = {Bojarski, Mariusz and {Del Testa}, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
eprint = {1604.07316},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf:pdf},
month = {apr},
title = {{End to End Learning for Self-Driving Cars}},
url = {http://arxiv.org/abs/1604.07316},
year = {2016}
}
@article{Bracci2017,
abstract = {The dorsal, parietal visual stream is activated when seeing objects, but the exact nature of parietal object representations is still under discussion. Here we test 2 specific hypotheses. First, parietal cortex is biased to host some representations more than others, with a different bias compared with ventral areas. A prime example would be object action representations. Second, parietal cortex forms a general multiple-demand network with frontal areas, showing similar task effects and representational content compared with frontal areas. To differentiate between these hypotheses, we implemented a human neuroimaging study with a stimulus set that dissociates associated object action from object category while manipulating task context to be either action- or category-related. Representations in parietal as well as prefrontal areas represented task-relevant object properties (action representations in the action task), with no sign of the irrelevant object property (category representations in the action task). In contrast, irrelevant object properties were represented in ventral areas. These findings emphasize that human parietal cortex does not preferentially represent particular object properties irrespective of task, but together with frontal areas is part of a multiple-demand and content-rich cortical network representing task-relevant object properties.},
author = {Bracci, Stefania and Daniels, Nicky and {Op de Beeck}, Hans},
doi = {10.1093/cercor/bhw419},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Bracci, Daniels, Op de Beeck - 2017 - Task Context Overrules Object- and Category-Related Representational Content in the Human Parietal.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral cortex (New York, N.Y. : 1991)},
keywords = {dorsal stream,fMRI,object representations,representational similarity analysis,ventral stream},
number = {1},
pages = {310--321},
pmid = {28108492},
publisher = {Oxford University Press},
title = {{Task Context Overrules Object- and Category-Related Representational Content in the Human Parietal Cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28108492 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5939221},
volume = {27},
year = {2017}
}
@misc{Brashers-Krug1994,
author = {Brashers-Krug, Tom and Shadmehr, Reza and Todorov, Emanuel},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Brashers-Krug, Shadmehr, Todorov - 1994 - Catastrophic Interference in Human Motor Learning.pdf:pdf},
pages = {19--26},
title = {{Catastrophic Interference in Human Motor Learning}},
url = {https://papers.nips.cc/paper/973-catastrophic-interference-in-human-motor-learning},
year = {1994}
}
@article{Brown1988a,
abstract = {In a series of seven experiments we examined preschool children's ability to learn and transfer across problems that shared a common underlying structure but differed in surface manifestations. The problems involved novel uses of familiar tools or simple biological themes such as mimicry as a method of defense. In the first three studies, we examined children's ability to learn to transfer after being exposed to a variety of transfer situations. Three-year-olds benefit from conditions that encourage them to reflect upon relational similarity; four-year-olds show a learning to learn effect without prompts to look for similarity. Both ages rapidly form a mind set to look for analogous solutions across problems. In Studies 4 to 7, we looked at preschoolers' learning from examples. When required to explain why an example is an illustration of a general theme, transfer to other instances of that theme is rapid, often occurring on the basis of only one example. Explanations and elaborations provided by the children, either spontaneously or in response to prompts, are much more effective at promoting transfer than those provided by an experimenter. The data are discussed in terms of explanation or analysis-based models of both machine and human learning.},
author = {Brown, Ann L and Kane, Mary Jo},
doi = {10.1016/0010-0285(88)90014-X},
issn = {0010-0285},
journal = {Cognitive Psychology},
month = {oct},
number = {4},
pages = {493--523},
publisher = {Academic Press},
title = {{Preschool children can learn to transfer: Learning to learn and learning from example}},
url = {https://www-sciencedirect-com.ccl.idm.oclc.org/science/article/pii/001002858890014X},
volume = {20},
year = {1988}
}
@article{Caruna1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Caruna, Rich},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Caruna - 1997 - Multitask Learning.pdf:pdf},
journal = {Machine Learning},
pages = {41--75},
publisher = {Kluwer Academic Publishers},
title = {{Multitask Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.8707{\&}rep=rep1{\&}type=pdf},
volume = {28},
year = {1997}
}
@techreport{Chandu2018,
abstract = {Problems at the intersection of language and vision, like visual question answering , have recently been gaining a lot of attention in the field of multi-modal machine learning as computer vision research moves beyond traditional recognition tasks. There has been recent success in visual question answering using deep neural network models which use the linguistic structure of the questions to dynamically instantiate network layouts. In the process of converting the question to a network layout, the question is simplified, which results in loss of information in the model. In this paper, we enrich the image information with textual data using image captions and external knowledge bases to generate more coherent answers. We achieve 57.1{\%} overall accuracy on the test-dev open-ended questions from the visual question answering (VQA 1.0) real image dataset.},
archivePrefix = {arXiv},
arxivId = {1809.08697v1},
author = {Chandu, Khyathi and {Arpita Pyreddy}, Mary and Felix, Matthieu and {Nath Joshi}, Narendra},
eprint = {1809.08697v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Chandu et al. - Unknown - Textually Enriched Neural Module Networks for Visual Question Answering.pdf:pdf},
title = {{Textually Enriched Neural Module Networks for Visual Question Answering}},
url = {https://arxiv.org/pdf/1809.08697.pdf},
year = {2018}
}
@techreport{Chen,
abstract = {The concept of conditional computation for deep nets has been proposed previously to improve model performance by selectively using only parts of the model conditioned on the sample it is processing. In this paper, we investigate input-dependent dynamic filter selection in deep convolutional neural networks (CNNs). The problem is interesting because the idea of forcing different parts of the model to learn from different types of samples may help us acquire better filters in CNNs, improve the model generalization performance and potentially increase the inter-pretability of model behavior. We propose a novel yet simple framework called GaterNet, which involves a backbone and a gater network. The backbone network is a regular CNN that performs the major computation needed for making a prediction, while a global gater network is introduced to generate binary gates for selectively activating filters in the backbone network based on each input. Extensive experiments on CIFAR and ImageNet datasets show that our models consistently outperform the original models with a large margin. On CIFAR-10, our model also improves upon state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1811.11205v1},
author = {Chen, Zhourong and Li, Yang and Bengio, Samy and Si, Si},
eprint = {1811.11205v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - GaterNet Dynamic Filter Selection in Convolutional Neural Network via a Dedicated Global Gating Network.pdf:pdf},
title = {{GaterNet: Dynamic Filter Selection in Convolutional Neural Network via a Dedicated Global Gating Network}},
url = {https://arxiv.org/pdf/1811.11205.pdf},
year = {2018}
}
@article{Cukur2013,
abstract = {The authors use functional magnetic resonance imaging to measure how the semantic representation changes when searching for different object categories in natural movies. They find tuning shifts that expand the representation of the attended category and of semantically related, but unattended, categories, and compress the representation of categories semantically dissimilar to the target.},
author = {{\c{C}}ukur, Tolga and Nishimoto, Shinji and Huth, Alexander G and Gallant, Jack L},
doi = {10.1038/nn.3381},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/{\c{C}}ukur et al. - 2013 - Attention during natural vision warps semantic representation across the human brain.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Attention,Neural encoding,Object vision,Perception},
month = {jun},
number = {6},
pages = {763--770},
publisher = {Nature Publishing Group},
title = {{Attention during natural vision warps semantic representation across the human brain}},
url = {http://www.nature.com/articles/nn.3381},
volume = {16},
year = {2013}
}
@inproceedings{Deng2009,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {248--255},
publisher = {IEEE},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://ieeexplore.ieee.org/document/5206848/},
year = {2009}
}
@techreport{Devlin2018,
author = {Devlin, Jacob},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Devlin - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding (Bidirectional Encoder Representations f.pdf:pdf},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Bidirectional Encoder Representations from Transformers)}},
url = {https://nlp.stanford.edu/seminar/details/jdevlin.pdf?utm{\_}campaign=NLP News{\&}utm{\_}medium=email{\&}utm{\_}source=Revue newsletter},
year = {2018}
}
@article{Devlin2018a,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7 (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5{\%} absolute improvement), outperforming human performance by 2.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{Donner2015,
author = {Donner, Yoni and Hardy, Joseph L.},
doi = {10.3758/s13423-015-0811-x},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {oct},
number = {5},
pages = {1308--1319},
publisher = {Springer US},
title = {{Piecewise power laws in individual learning curves}},
url = {http://link.springer.com/10.3758/s13423-015-0811-x},
volume = {22},
year = {2015}
}
@techreport{Duan2017,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL 2 , the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations , actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL 2 experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-armed bandit problems and finite MDPs. After RL 2 is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL 2 on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.02779v2},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
eprint = {arXiv:1611.02779v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Duan et al. - 2017 - RL2 Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
institution = {UC Berkeley},
title = {{RL{\^{}}2: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {https://arxiv.org/pdf/1611.02779.pdf},
year = {2017}
}
@article{Dumoulin2018,
abstract = {Many real-world problems require integrating multiple sources of information. Sometimes these problems involve multiple, distinct modalities of information — vision, language, audio, etc. — as is required to understand a scene in a movie or answer a question about an image. Other times, these problems involve multiple sources of the same kind of input, i.e. when summarizing several documents or drawing one image in the style of another.

When approaching such problems, it often makes sense to process one source of information in the context of another; for instance, in the right example above, one can extract meaning from the image in the context of the question. In machine learning, we often refer to this context-based processing as conditioning: the computation carried out by a model is conditioned or modulated by information extracted from an auxiliary input.

Finding an effective way to condition on or fuse sources of information is an open research problem, and in this article, we concentrate on a specific family of approaches we call feature-wise transformations. We will examine the use of feature-wise transformations in many neural network architectures to solve a surprisingly large and diverse set of problems; their success, we will argue, is due to being flexible enough to learn an effective representation of the conditioning input in varied settings. In the language of multi-task learning, where the conditioning signal is taken to be a task description, feature-wise transformations learn a task representation which allows them to capture and leverage the relationship between multiple sources of information, even in remarkably different problem settings.},
author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm and Courville, Aaron and Bengio, Yoshua},
doi = {10.23915/distill.00011},
journal = {Distill},
month = {jul},
number = {7},
pages = {e11},
title = {{Feature-wise transformations}},
url = {https://distill.pub/2018/feature-wise-transformations},
volume = {3},
year = {2018}
}
@article{LiFei-Fei2006,
abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
author = {Fei-Fei, Li and Fergus, R. and Perona, P.},
doi = {10.1109/TPAMI.2006.79},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {apr},
number = {4},
pages = {594--611},
title = {{One-shot learning of object categories}},
url = {http://ieeexplore.ieee.org/document/1597116/ http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf},
volume = {28},
year = {2006}
}
@article{FeiFei2002,
abstract = {What can we see when we do not pay attention? It is well known that we can be "blind" even to major aspects of natural scenes when we attend elsewhere. The only tasks that do not need attention appear to be carried out in the early stages of the visual system. Contrary to this common belief, we report that subjects can rapidly detect animals or vehicles in briefly presented novel natural scenes while simultaneously performing another attentionally demanding task. By comparison, they are unable to discriminate large T's from L's, or bisected two-color disks from their mirror images under the same conditions. We conclude that some visual tasks associated with "high-level" cortical areas may proceed in the near absence of attention.},
author = {Fei-Fei, Li and VanRullen, Rufin and Koch, Christof and Perona, Pietro},
doi = {10.1073/pnas.092277599},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Fei Fei et al. - 2002 - Rapid natural scene categorization in the near absence of attention.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {jul},
number = {14},
pages = {9596--601},
pmid = {12077298},
publisher = {National Academy of Sciences},
title = {{Rapid natural scene categorization in the near absence of attention.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12077298},
volume = {99},
year = {2002}
}
@techreport{Fernando2018,
abstract = {The scope of the Baldwin effect was recently called into question by two papers that closely examined the seminal work of Hinton and Nowlan. To this date there has been no demonstration of its necessity in empirically challenging tasks. Here we show that the Baldwin effect is capable of evolving few-shot supervised and reinforcement learning mechanisms, by shaping the hyperparameters and the initial parameters of deep learning algorithms. Furthermore it can genetically accommodate strong learning biases on the same set of problems as a recent machine learning algorithm called MAML "Model Agnostic Meta-Learning" which uses second-order gradients instead of evolution to learn a set of reference parameters (initial weights) that can allow rapid adaptation to tasks sampled from a distribution. Whilst in simple cases MAML is more data efficient than the Baldwin effect, the Baldwin effect is more general in that it does not require gradients to be backpropagated to the reference parameters or hyperparameters, and permits effectively any number of gradient updates in the inner loop. The Baldwin effect learns strong learning dependent biases, rather than purely genetically accommodating fixed behaviours in a learning independent manner.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.07917v2},
author = {Fernando, Chrisantha and Sygnowski, Jakub and Osindero, Simon and Wang, Jane and Schaul, Tom and Teplyashin, Denis and Sprechmann, Pablo and Pritzel, Alexander and Rusu, Andrei A},
eprint = {arXiv:1806.07917v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Fernando et al. - Unknown - Meta-Learning by the Baldwin Effect.pdf:pdf},
title = {{Meta-Learning by the Baldwin Effect}},
url = {https://arxiv.org/pdf/1806.07917.pdf},
year = {2018}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is com-patible with any model trained with gradient de-scent and applicable to a variety of different learning problems, including classification, re-gression, and reinforcement learning. The goal of meta-learning is to train a model on a vari-ety of learning tasks, such that it can solve new learning tasks using only a small number of train-ing samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and acceler-ates fine-tuning for policy gradient reinforcement learning with neural network policies.},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks(2).pdf:pdf},
journal = {Proceedings of the 34th International Conference on Machine Learning,},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {https://arxiv.org/pdf/1703.03400.pdf},
volume = {70},
year = {2017}
}
@techreport{Finn2018,
abstract = {Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universal-ity, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning al-gorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11622v3},
author = {Finn, Chelsea and Levine, Sergey},
eprint = {arXiv:1710.11622v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Levine - 2018 - Meta-Learning and Universality Deep Representations and Gradient Descent can Approximate any Learning Algorithm.pdf:pdf},
title = {{Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm}},
url = {https://arxiv.org/pdf/1710.11622.pdf},
year = {2018}
}
@techreport{Finn2018a,
abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.02817v1},
author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
eprint = {arXiv:1806.02817v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Finn, Xu, Levine - 2018 - Probabilistic Model-Agnostic Meta-Learning.pdf:pdf},
title = {{Probabilistic Model-Agnostic Meta-Learning}},
url = {https://arxiv.org/pdf/1806.02817.pdf},
year = {2018}
}
@article{French1999,
abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically'. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
author = {French, Robert M.},
doi = {10.1016/S1364-6613(99)01294-2},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
month = {apr},
number = {4},
pages = {128--135},
publisher = {Elsevier Current Trends},
title = {{Catastrophic forgetting in connectionist networks}},
url = {https://www.sciencedirect.com/science/article/pii/S1364661399012942},
volume = {3},
year = {1999}
}
@article{Garg2016,
abstract = {We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning. Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks. We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering.},
archivePrefix = {arXiv},
arxivId = {1612.09030},
author = {Garg, Vikas K. and Kalai, Adam},
eprint = {1612.09030},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Garg, Kalai - 2016 - Meta-Unsupervised-Learning A supervised approach to unsupervised learning.pdf:pdf},
title = {{Meta-Unsupervised-Learning: A supervised approach to unsupervised learning}},
url = {http://arxiv.org/abs/1612.09030},
year = {2017}
}
@inproceedings{Garg2018,
abstract = {We introduce a framework to leverage knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via simple agnostic bounds on unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm, remove the outliers, and provably circumvent the Kleinberg's impossibility result. Experimental results across hundreds of problems demonstrate improved performance on unsupervised data with simple algorithms, despite the fact that our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features from many such small datasets, and perform zero shot learning.},
archivePrefix = {arXiv},
arxivId = {1709.05262},
author = {Garg, Vikas K. and Kalai, Adam},
booktitle = {NIPS},
eprint = {1709.05262},
file = {::},
title = {{Supervising Unsupervised Learning}},
url = {http://arxiv.org/abs/1709.05262},
year = {2018}
}
@article{Goodfellow2015,
abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
archivePrefix = {arXiv},
arxivId = {1312.6211},
author = {Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
eprint = {1312.6211},
title = {{An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks}},
url = {http://arxiv.org/abs/1312.6211},
year = {2015}
}
@article{Gopnik,
abstract = {Three studies explored whether and when children could categorize objects on the basis of a novel underlying causal power. To test this we constructed a "blicket detector," a machine that lit up and played music when certain objects were placed on it. First, 2-, 3- and 4-year-old children saw that an object labeled as a "blicket" would set off the machine. In a categorization task, other objects were demonstrated on the machine. Some set it off and some did not. Children were asked to say which objects were "blickets." In an induction task, other objects were or were not labeled as "blickets." Children had to predict which objects would have the causal power to set off the machine. The causal power could conflict with perceptual properties of the object, such as color and shape. In an association task the object was associated with the machine's lighting up but did not cause it to light up. Even the youngest children sometimes used the causal power to determine the object's name rather than using its perceptual properties and sometimes used the object's name rather than its perceptual properties to predict the object's causal powers. Children rarely categorized the object on the basis of the associated event. Young children also sometimes made interesting memory errors-they incorrectly reported that objects with the same perceptual features had had the same causal power. These studies demonstrate that even very young children will easily and swiftly learn about a new causal power of an object and spontaneously use that information in classifying and naming the object.},
author = {Gopnik, A and Sobel, D M},
journal = {Child development},
number = {5},
pages = {1205--22},
pmid = {11108092},
title = {{Detecting blickets: how young children use information about novel causal powers in categorization and induction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11108092 https://www.researchgate.net/profile/David{\_}Sobel/publication/239666424{\_}Detecting{\_}blickets{\_}How{\_}young{\_}children{\_}use{\_}information{\_}about{\_}causal{\_}properties{\_}in{\_}categorization{\_}and{\_}induction/links/00b4952c427c668f96000000},
volume = {71},
year = {2000}
}
@techreport{Grant2018,
abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1801.08930v1},
author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
eprint = {arXiv:1801.08930v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Grant et al. - 2018 - Recasting Gradient-Based Meta-Learning as Hierarchical Bayes.pdf:pdf},
title = {{Recasting Gradient-Based Meta-Learning as Hierarchical Bayes}},
url = {https://arxiv.org/pdf/1801.08930.pdf},
year = {2018}
}
@article{Green2008,
abstract = {Human beings have an amazing capacity to learn new skills and adapt to new environments. However, several obstacles remain to be overcome in designing paradigms to broadly improve quality of life. Arguably, the most notable impediment to this goal is that learning tends to be quite specific to the trained regimen and does not transfer to even qualitatively similar tasks. This severely limits the potential benefits of learning to daily life. This review discusses training regimens that lead to the acquisition of new knowledge and strategies that can be used flexibly across a range of tasks and contexts. Possible characteristics of training regimens are proposed that may be responsible for augmented learning, including the manner in which task difficulty is progressed, the motivational state of the learner, and the type of feedback the training provides. When maximally implemented in rehabilitative paradigms, these characteristics may greatly increase the efficacy of training.},
author = {Green, C S and Bavelier, D},
doi = {10.1037/a0014345},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Green, Bavelier - 2008 - Exercising your brain a review of human brain plasticity and training-induced learning.pdf:pdf},
issn = {0882-7974},
journal = {Psychology and aging},
month = {dec},
number = {4},
pages = {692--701},
pmid = {19140641},
publisher = {NIH Public Access},
title = {{Exercising your brain: a review of human brain plasticity and training-induced learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19140641 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2896818},
volume = {23},
year = {2008}
}
@incollection{Greene2008,
abstract = {The study of Learning and Memory is a central topic in Neuroscience and Psychology. It is also a very good example of a field that has come into maturity on all levels - in the protein chemistry and molecular biology of the cellular events underlying learning and memory, the properties and functions of neuronal networks, the psychology and behavioural neuroscience of learning and memory. Many of the basic research findings are directly applicable in the treatment of diseases and aging phenomena, and have found their way into educational theory and praxis.

Learning and Memory: A Comprehensive Reference is the most comprehensive source of information about learning and memory ever assembled, and the definitive reference work on the topic. In four volumes, Editor-in-Chief John H. Byrne (University of Texas), together with volume editors Howard Eichenbaum (Boston University) for Systems and Neuroscience, Randolf Menzel (Freie Universit{\"{a}}t Berlin) for Behavioral Approaches, Henry Roediger (Washington University) for Cognitive Psychology, and David Sweatt (University of Alabama, Birmingham) for Molecular Mechanisms, have put together a truly authoritative collection of overview articles in 159 chapters on over 3000 pages. Learning and Memory: A Comprehensive Reference presents an extensive, integrated summary of the present state of research in the neurobiology and psychology of learning and memory and covers an enormous range of intellectual territory. With topics ranging from the neurochemistry and neurobiology of learning at the cellular and synaptic levels, systems neurobiology, the study of remarkable capabilities in animals (such as homing), ethological and behavioristic analyses, mechanisms, psychology, and disorders of learning and memory in humans, the work broadly covers all topics in the neurobiology and psychology of learning and memory. There is no other handbook with such a comprehensive coverage and depth. The authors selected are the leading scholars for the particular topics on which they write.},
address = {Oxford},
author = {Greene, R. L.},
booktitle = {Learning and memory: a comprehensive reference.  Vol. 2: Cognitive psychology of memory },
edition = {1st},
editor = {Roediger, H. L. III},
isbn = {9780123705044},
pages = {65--78},
publisher = {Elsevier},
title = {{Repetition and spacing effects}},
year = {2008}
}
@article{Griffiths2011,
abstract = {People are adept at inferring novel causal relations, even from only a few observations. Prior knowledge about the probability of encountering causal relations of various types and the nature of the mechanisms relating causes and effects plays a crucial role in these inferences. We test a formal account of how this knowledge can be used and acquired, based on analyzing causal induction as Bayesian inference. Five studies explored the predictions of this account with adults and 4-year-olds, using tasks in which participants learned about the causal properties of a set of objects. The studies varied the two factors that our Bayesian approach predicted should be relevant to causal induction: the prior probability with which causal relations exist, and the assumption of a deterministic or a probabilistic relation between cause and effect. Adults' judgments (Experiments 1, 2, and 4) were in close correspondence with the quantitative predictions of the model, and children's judgments (Experiments 3 and 5) agreed qualitatively with this account.},
author = {Griffiths, Thomas L and Sobel, David M and Tenenbaum, Joshua B and Gopnik, Alison},
doi = {10.1111/j.1551-6709.2011.01203.x},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths et al. - 2011 - Bayes and blickets effects of knowledge on causal induction in children and adults.pdf:pdf},
journal = {Cognitive science},
number = {8},
pages = {1407--55},
pmid = {21972897},
publisher = {NIH Public Access},
title = {{Bayes and blickets: effects of knowledge on causal induction in children and adults.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21972897 https://onlinelibrary-wiley-com.ccl.idm.oclc.org/doi/pdf/10.1111/j.1551-6709.2011.01203.x},
volume = {35},
year = {2011}
}
@techreport{Hamrick2018,
abstract = {While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a "re-lational inductive bias": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object-and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more na¨ıvena¨ıve approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.01203v1},
author = {Hamrick, Jessica B and Allen, Kelsey R and Bapst, Victor and Zhu, Tina and Mckee, Kevin R and Battaglia, Peter W},
eprint = {arXiv:1806.01203v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hamrick et al. - 2018 - Relational inductive bias for physical construction in humans and machines.pdf:pdf},
title = {{Relational inductive bias for physical construction in humans and machines}},
url = {https://goo.gl/f7Ecw8},
year = {2018}
}
@inproceedings{Hariharan2016,
abstract = {Representation learning Low-shot learning Feature extractor Base classes (many training examples) Classifier (base and novel categories) Novel classes (few training examples) Figure 1: Our low-shot learning benchmark in two phases: representation learning and low-shot learning. Modern recognition models use large labeled datasets like ImageNet to build good visual representations and train strong classifiers (representation learning). However, these datasets only contain a fixed set of classes. In many realistic scenarios, once deployed, the model might encounter novel classes that it also needs to recognize, but with very few training examples available (low-shot learning). We present two ways of significantly improving performance in this scenario: (1) a novel loss function for representation learning that leads to better visual representations that generalize well, and (2) a method for hallucinating additional examples for the data-starved novel classes. Abstract Low-shot visual learning-the ability to recognize novel object categories from very few examples-is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3× on the challenging ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02819v4},
author = {Hariharan, Bharath and Girshick, Ross},
booktitle = {ICCV},
eprint = {1606.02819v4},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hariharan, Girshick - Unknown - Low-shot Visual Recognition by Shrinking and Hallucinating Features.pdf:pdf},
title = {{Low-shot Visual Recognition by Shrinking and Hallucinating Features}},
url = {https://arxiv.org/pdf/1606.02819.pdf},
year = {2017}
}
@article{Harrison2018,
abstract = {Gaussian Process (GP) regression has seen widespread use in robotics due to its generality, simplicity of use, and the utility of Bayesian predictions. The predominant implementation of GP regression is a nonparameteric kernel-based approach, as it enables fitting of arbitrary nonlinear functions. However, this approach suffers from two main drawbacks: (1) it is computationally inefficient, as computation scales poorly with the number of samples; and (2) it can be data inefficient, as encoding prior knowledge that can aid the model through the choice of kernel and associated hyperparameters is often challenging and unintuitive. In this work, we propose ALPaCA, an algorithm for efficient Bayesian regression which addresses these issues. ALPaCA uses a dataset of sample functions to learn a domain-specific, finite-dimensional feature encoding, as well as a prior over the associated weights, such that Bayesian linear regression in this feature space yields accurate online predictions of the posterior predictive density. These features are neural networks, which are trained via a meta-learning (or "learning-to-learn") approach. ALPaCA extracts all prior information directly from the dataset, rather than restricting prior information to the choice of kernel hyperparameters. Furthermore, by operating in the weight space, it substantially reduces sample complexity. We investigate the performance of ALPaCA on two simple regression problems, two simulated robotic systems, and on a lane-change driving task performed by humans. We find our approach outperforms kernel-based GP regression, as well as state of the art meta-learning approaches, thereby providing a promising plug-in tool for many regression tasks in robotics where scalability and data-efficiency are important.},
archivePrefix = {arXiv},
arxivId = {1807.08912},
author = {Harrison, James and Sharma, Apoorva and Pavone, Marco},
eprint = {1807.08912},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Harrison, Sharma, Pavone - 2018 - Meta-Learning Priors for Efficient Online Bayesian Regression.pdf:pdf},
month = {jul},
title = {{Meta-Learning Priors for Efficient Online Bayesian Regression}},
url = {http://arxiv.org/abs/1807.08912},
year = {2018}
}
@article{Hermundstad2011,
abstract = {The performance of information processing systems, from artificial neural networks to natural neuronal ensembles, depends heavily on the underlying system architecture. In this study, we compare the performance of parallel and layered network architectures during sequential tasks that require both acquisition and retention of information, thereby identifying tradeoffs between learning and memory processes. During the task of supervised, sequential function approximation, networks produce and adapt representations of external information. Performance is evaluated by statistically analyzing the error in these representations while varying the initial network state, the structure of the external information, and the time given to learn the information. We link performance to complexity in network architecture by characterizing local error landscape curvature. We find that variations in error landscape structure give rise to tradeoffs in performance; these include the ability of the network to maximize accuracy versus minimize inaccuracy and produce specific versus generalizable representations of information. Parallel networks generate smooth error landscapes with deep, narrow minima, enabling them to find highly specific representations given sufficient time. While accurate, however, these representations are difficult to generalize. In contrast, layered networks generate rough error landscapes with a variety of local minima, allowing them to quickly find coarse representations. Although less accurate, these representations are easily adaptable. The presence of measurable performance tradeoffs in both layered and parallel networks has implications for understanding the behavior of a wide variety of natural and artificial learning systems.},
author = {Hermundstad, Ann M. and Brown, Kevin S. and Bassett, Danielle S. and Carlson, Jean M.},
doi = {10.1371/journal.pcbi.1002063},
editor = {Sporns, Olaf},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hermundstad et al. - 2011 - Learning, Memory, and the Role of Neural Network Architecture.pdf:pdf},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {jun},
number = {6},
pages = {e1002063},
publisher = {Public Library of Science},
title = {{Learning, Memory, and the Role of Neural Network Architecture}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1002063},
volume = {7},
year = {2011}
}
@techreport{Hochreiter2001,
abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of "meta-learning," i.e. of a system that improves or discovers a learning algorithms, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their atendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
author = {Hochreiter, Sepp and Younger, A. Steven. and Conwell, Peter R.},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Younger, Conwell - 2001 - Learning to Learn Using Gradient Descent.pdf:pdf},
title = {{Learning to Learn Using Gradient Descent}},
url = {http://snowedin.net/tmp/Hochreiter2001.pdf},
year = {2001}
}
@techreport{Houthooft2018,
abstract = {We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal con-volutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms .},
archivePrefix = {arXiv},
arxivId = {1802.04821v2},
author = {Houthooft, Rein and Chen, Richard Y and Isola, Phillip and Stadie, Bradly C and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
eprint = {1802.04821v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Houthooft et al. - Unknown - Evolved Policy Gradients.pdf:pdf},
title = {{Evolved Policy Gradients}},
url = {http://github.com/openai/EPG.},
year = {2018}
}
@inproceedings{Hsu2019,
abstract = {A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.},
archivePrefix = {arXiv},
arxivId = {1810.02334},
author = {Hsu, Kyle and Levine, Sergey and Finn, Chelsea},
booktitle = {ICLR},
eprint = {1810.02334},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hsu, Levine, Finn - 2019 - Unsupervised Learning via Meta-Learning.pdf:pdf},
title = {{Unsupervised Learning via Meta-Learning}},
url = {http://arxiv.org/abs/1810.02334},
year = {2019}
}
@techreport{Hu2017,
abstract = {Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer "is there an equal number of balls and boxes?" we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach to question answering by parsing questions into linguistic sub-structures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at com-positional question answering show that N2NMNs achieve an error reduction of nearly 50{\%} relative to state-of-the-art attentional approaches, while discovering interpretable network architectures specialized for each question.},
archivePrefix = {arXiv},
arxivId = {arXiv:1704.05526v3},
author = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
eprint = {arXiv:1704.05526v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2017 - Learning to Reason End-to-End Module Networks for Visual Question Answering.pdf:pdf},
title = {{Learning to Reason: End-to-End Module Networks for Visual Question Answering}},
url = {https://arxiv.org/pdf/1704.05526.pdf},
year = {2017}
}
@techreport{Hu2016,
abstract = {People often refer to entities in an image in terms of their relationships with other entities. For example, the black cat sitting under the table refers to both a black cat entity and its relationship with another table entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referen-tial expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neu-ral modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple ref-erential expression datasets, outperforming state-of-the-art approaches on all tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.09978v1},
author = {Hu, Ronghang and Rohrbach, Marcus and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
eprint = {arXiv:1611.09978v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2016 - Modeling Relationships in Referential Expressions with Compositional Modular Networks.pdf:pdf},
title = {{Modeling Relationships in Referential Expressions with Compositional Modular Networks}},
url = {https://arxiv.org/pdf/1611.09978.pdf},
year = {2016}
}
@article{Hu2018,
abstract = {Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1611.02041},
author = {Hu, Weihua and Niu, Gang and Sato, Issei and Sugiyama, Masashi},
eprint = {1611.02041},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2018 - Does Distributionally Robust Supervised Learning Give Robust Classifiers.pdf:pdf},
title = {{Does Distributionally Robust Supervised Learning Give Robust Classifiers?}},
url = {http://arxiv.org/abs/1611.02041},
year = {2018}
}
@article{Hunter2007,
abstract = {Matplotlib is a 2D graphics package used for Python
for application development, interactive scripting, and
publication-quality image generation across user
interfaces and operating systems.},
author = {Hunter, J D},
doi = {10.1109/MCSE.2007.55},
journal = {Computing In Science {\&} Engineering},
number = {3},
pages = {90--95},
publisher = {IEEE COMPUTER SOC},
title = {{Matplotlib: A 2D graphics environment}},
volume = {9},
year = {2007}
}
@misc{InteractiveStatisticalPages,
author = {{Interactive Statistical Pages}},
title = {{Latin Squares - Williams Design}},
url = {http://statpages.info/latinsq.html}
}
@inproceedings{Johnson2017,
abstract = {When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
archivePrefix = {arXiv},
arxivId = {1612.06890v1},
author = {Johnson, Justin and Fei-Fei, Li and Hariharan, Bharath and Zitnick, C Lawrence and {Van Der Maaten}, Laurens and Girshick, Ross},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1612.06890v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - 2017 - CLEVR A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning.pdf:pdf},
isbn = {1612.06890v1},
title = {{CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning}},
url = {https://arxiv.org/pdf/1612.06890.pdf},
year = {2017}
}
@techreport{Johnson,
abstract = {Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.},
archivePrefix = {arXiv},
arxivId = {1705.03633v1},
author = {Johnson, Justin and Hariharan, Bharath and {Van Der Maaten}, Laurens and Hoffman, Judy and Fei-Fei, Li and Zitnick, C Lawrence and Girshick, Ross},
eprint = {1705.03633v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - Unknown - Inferring and Executing Programs for Visual Reasoning.pdf:pdf},
title = {{Inferring and Executing Programs for Visual Reasoning}},
url = {https://arxiv.org/pdf/1705.03633.pdf},
year = {2017}
}
@techreport{Kahou2017,
abstract = {We present an attention-based modular neural framework for computer vision. The framework uses a soft attention mechanism allowing models to be trained with gradient descent. It consists of three modules: a recurrent attention module controlling where to look in an image or video frame, a feature-extraction module providing a representation of what is seen, and an objective module formalizing why the model learns its attentive behavior. The attention module allows the model to focus computation on task-related information in the input. We apply the framework to several object tracking tasks and explore various design choices. We experiment with three data sets, bouncing ball, moving digits and the real-world KTH data set. The proposed Recurrent Attentional Tracking Model (RATM) performs well on all three tasks and can generalize to related but previously unseen sequences from a challenging tracking data set.},
author = {Kahou, Samira Ebrahimi and Michalski, Vincent and Memisevic, Roland and Pal, Christopher and Vincent, Pascal},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kahou et al. - 2017 - RATM Recurrent Attentive Tracking Model.pdf:pdf},
title = {{RATM: Recurrent Attentive Tracking Model}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017{\_}workshops/w20/papers/Kahou{\_}RATM{\_}Recurrent{\_}Attentive{\_}CVPR{\_}2017{\_}paper.pdf},
year = {2017}
}
@article{Kamra2017,
abstract = {Despite advances in deep learning, neural networks can only learn multiple tasks when trained on them jointly. When tasks arrive sequentially, they lose performance on previously learnt tasks. This phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our contributions are: (i) a dual memory architecture emulating the complementary learning systems (hippocampus and the neocortex) in the human brain, (ii) memory consolidation via generative replay of past experiences, (iii) demonstrating advantages of generative replay and dual memories via experiments, and (iv) improved performance retention on challenging tasks even for low capacity models. Our architecture displays many characteristics of the mammalian memory and provides insights on the connection between sleep and learning.},
archivePrefix = {arXiv},
arxivId = {1710.10368},
author = {Kamra, Nitin and Gupta, Umang and Liu, Yan},
eprint = {1710.10368},
file = {::},
month = {oct},
title = {{Deep Generative Dual Memory Network for Continual Learning}},
url = {http://arxiv.org/abs/1710.10368},
year = {2017}
}
@article{Kay2015,
abstract = {Ventral temporal cortex (VTC) is the latest stage of the ventral “what” visual pathway, which is thought to code the identity of a stimulus regardless of its position or size [1, 2]. Surprisingly, recent studies show that position information can be decoded from VTC [3–5]. However, the computational mechanisms by which spatial information is encoded in VTC are unknown. Furthermore, how attention influences spatial representations in human VTC is also unknown because the effect of attention on spatial representations has only been examined in the dorsal “where” visual pathway [6–10]. Here, we fill these significant gaps in knowledge using an approach that combines functional magnetic resonance imaging and sophisticated computational methods. We first develop a population receptive field (pRF) model [11, 12] of spatial responses in human VTC. Consisting of spatial summation followed by a compressive nonlinearity, this model accurately predicts responses of individual voxels to stimuli at any position and size, explains how spatial information is encoded, and reveals a functional hierarchy in VTC. We then manipulate attention and use our model to decipher the effects of attention. We find that attention to the stimulus systematically and selectively modulates responses in VTC, but not early visual areas. Locally, attention increases eccentricity, size, and gain of individual pRFs, thereby increasing position tolerance. However, globally, these effects reduce uncertainty regarding stimulus location and actually increase position sensitivity of distributed responses across VTC. These results demonstrate that attention actively shapes and enhances spatial representations in the ventral visual pathway.},
author = {Kay, Kendrick N and Weiner, Kevin S and Grill-Spector, Kalanit},
doi = {10.1016/J.CUB.2014.12.050},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kay, Weiner, Grill-Spector - 2015 - Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex.pdf:pdf},
issn = {0960-9822},
journal = {Current Biology},
month = {mar},
number = {5},
pages = {595--600},
publisher = {Cell Press},
title = {{Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex}},
url = {https://www-sciencedirect-com.ccl.idm.oclc.org/science/article/pii/S0960982214016534{\#}!},
volume = {25},
year = {2015}
}
@inproceedings{Kemker2018,
abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
archivePrefix = {arXiv},
arxivId = {1711.10563},
author = {Kemker, Ronald and Kanan, Christopher},
booktitle = {ICLR},
eprint = {1711.10563},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kemker, Kanan - 2017 - FearNet Brain-Inspired Model for Incremental Learning.pdf:pdf},
month = {nov},
title = {{FearNet: Brain-Inspired Model for Incremental Learning}},
url = {http://arxiv.org/abs/1711.10563},
year = {2018}
}
@inproceedings{Kemker2017a,
abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than retraining the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regulariza-tion, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.},
archivePrefix = {arXiv},
arxivId = {1708.02072v4},
author = {Kemker, Ronald and Mcclure, Marc and Abitino, Angelina and Hayes, Tyler and Kanan, Christopher},
booktitle = {AAAI'18},
eprint = {1708.02072v4},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kemker et al. - 2017 - Measuring Catastrophic Forgetting in Neural Networks.pdf:pdf},
title = {{Measuring Catastrophic Forgetting in Neural Networks}},
url = {https://arxiv.org/pdf/1708.02072.pdf},
year = {2018}
}
@incollection{Khamassi2011,
author = {Khamassi, Mehdi and Wilson, Charles R. E. and Roth{\'{e}}, Marie and Quilodran, Ren{\'{e}} and Dominey, Peter F. and Procyk, Emmanuel},
booktitle = {Neural Basis of Motivational and Cognitive Control},
doi = {10.7551/mitpress/9780262016438.003.0019},
month = {oct},
pages = {350--369},
publisher = {The MIT Press},
title = {{Meta-Learning, Cognitive Control, and Physiological Interactions between Medial and Lateral Prefrontal Cortex}},
url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262016438.001.0001/upso-9780262016438-chapter-19},
year = {2011}
}
@article{Kim2019,
abstract = {One characteristic of human visual perception is the presence of `Gestalt phenomena,' that is, that the whole is something other than the sum of its parts. A natural question is whether image-recognition networks show similar effects. Our paper investigates one particular type of Gestalt phenomenon, the law of closure, in the context of a feedforward image classification neural network (NN). This is a robust effect in human perception, but experiments typically rely on measurements (e.g., reaction time) that are not available for artificial neural nets. We describe a protocol for identifying closure effect in NNs, and report on the results of experiments with simple visual stimuli. Our findings suggest that NNs trained with natural images do exhibit closure, in contrast to networks with randomized weights or networks that have been trained on visually random data. Furthermore, the closure effect reflects something beyond good feature extraction; it is correlated with the network's higher layer features and ability to generalize.},
archivePrefix = {arXiv},
arxivId = {1903.01069},
author = {Kim, Been and Reif, Emily and Wattenberg, Martin and Bengio, Samy},
eprint = {1903.01069},
file = {::},
month = {mar},
title = {{Do Neural Networks Show Gestalt Phenomena? An Exploration of the Law of Closure}},
url = {http://arxiv.org/abs/1903.01069},
year = {2019}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
booktitle = {Proceedings of the 3rd International Conference for Learning Representations},
eprint = {1412.6980},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A Method for Stochastic Optimization.pdf:pdf},
title = {{Adam: A Method for Stochastic Optimization}},
url = {https://arxiv.org/abs/1412.6980},
year = {2015}
}
@techreport{Kirkpatrick2017,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
archivePrefix = {arXiv},
arxivId = {1612.00796v2},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
eprint = {1612.00796v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {https://arxiv.org/pdf/1612.00796.pdf},
year = {2017}
}
@article{Krakauer2011,
abstract = {Recent studies of upper limb movements have provided insights into the computations, mechanisms, and taxonomy of human sensorimotor learning. Motor tasks differ with respect to how they weight different learning processes. These include adaptation, an internal-model based process that reduces sensory-prediction errors in order to return performance to pre-perturbation levels, use-dependent plasticity, and operant reinforcement. Visuomotor rotation and force-field tasks impose systematic errors and thereby emphasize adaptation. In skill learning tasks, which for the most part do not involve a perturbation, improved performance is manifest as reduced motor variability and probably depends less on adaptation and more on success-based exploration. Explicit awareness and declarative memory contribute, to varying degrees, to motor learning. The modularity of motor learning processes maps, at least to some extent, onto distinct brain structures.},
author = {Krakauer, John W and Mazzoni, Pietro},
doi = {10.1016/j.conb.2011.06.012},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
month = {aug},
number = {4},
pages = {636--644},
pmid = {21764294},
title = {{Human sensorimotor learning: adaptation, skill, and beyond}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21764294 https://linkinghub.elsevier.com/retrieve/pii/S0959438811001218},
volume = {21},
year = {2011}
}
@article{Krizhevsky2013,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {:Users/guydavidson/Downloads/capstone.bib:bib},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2013}
}
@inproceedings{Krogh1992,
abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
author = {Krogh, Anders and Hertz, John A.},
booktitle = {Proceedings of the 4th International Conference on Neural Information Processing Systems},
isbn = {1558602224},
pages = {950--957},
publisher = {Morgan Kaufmann Publishers},
title = {{A simple weight decay can improve generalization}},
url = {https://dl-acm-org.ccl.idm.oclc.org/citation.cfm?id=2987033 https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf},
year = {1992}
}
@article{Kruschke1993,
abstract = {Backpropagation (Rumelhart et al., 1986) was proposed as a general learning algorithm for multi-layer perceptrons. This article demonstrates that a standard version of backprop fails to attend selectively to input dimensions in the same way as humans, suffers catastrophic forgetting of previously learned associations when novel exemplars are trained, and can be overly sensitive to linear category boundaries. Another connectionist model, ALCOVE (Kruschke 1990, 1992), does not suffer those failures. Previous researchers identified these problems; the present article reports quantitative fits of the models to new human learning data. ALCOVE can be functionally approximated by a network that uses linear-sigmoid hidden nodes, like standard backprop. It is argued that models of human category learning should incorporate quasi-local representations and dimensional attention learning, as well as error-driven learning, to address simultaneously all three phenomena.},
author = {Kruschke, John K.},
doi = {10.1080/09540099308915683},
journal = {Connection Science},
month = {jan},
number = {1},
pages = {3--36},
publisher = {Taylor {\&} Francis Group},
title = {{Human Category Learning: Implications for Backpropagation Models}},
url = {https://www.tandfonline.com/doi/full/10.1080/09540099308915683 http://www.indiana.edu/{~}kruschke/articles/Kruschke1993CS.pdf},
volume = {5},
year = {1993}
}
@article{Kubilius2018,
abstract = {Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NAS-Net architectures, demonstrating increasingly better object cat-egorization performance and increasingly better explanatory power of both neural and behavioral responses. However, from the neuroscientist's point of view, the relationship between such very deep architectures and the ventral visual pathway is incomplete in at least two ways. On the one hand, current state-of-the-art ANNs appear to be too complex (e.g., now over 100 levels) compared with the relatively shallow cortical hierarchy (4-8 levels), which makes it difficult to map their elements to those in the ventral visual stream and to understand what they are doing. On the other hand, current state-of-the-art ANNs appear to be not complex enough in that they lack recurrent connections and the resulting neural response dynamics that are commonplace in the ventral visual stream. Here we describe our ongoing efforts to resolve both of these issues by developing a "CORnet" family of deep neural network architectures. Rather than just seeking high object recognition performance (as the state-of-the-art ANNs above), we instead try to reduce the model family to its most important elements and then gradually build new ANNs with recurrent and skip connections while monitoring both performance and the match between each new CORnet model and a large body of primate brain and behav-ioral data. We report here that our current best ANN model derived from this approach (CORnet-S) is among the top models on Brain-Score, a composite benchmark for comparing models to the brain, but is simpler than other deep ANNs in terms of the number of convolutions performed along the longest path of information processing in the model. All CORnet models are available at github.com/dicarlolab/CORnet, and we plan to update this manuscript and the available models in this family as they are produced. object recognition | deep neural networks | feedforward | recurrence},
author = {Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L K and Dicarlo, James J},
doi = {10.1101/408385},
title = {{CORnet: Modeling the Neural Mechanisms of Core Object Recognition}},
url = {http://dx.doi.org/10.1101/408385},
year = {2018}
}
@article{Kuznetsova2018,
abstract = {We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, and we study how the performance of many modern models evolves with increasing amounts of training data. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.},
archivePrefix = {arXiv},
arxivId = {1811.00982},
author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Duerig, Tom and Ferrari, Vittorio},
eprint = {1811.00982},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Kuznetsova et al. - 2018 - The Open Images Dataset V4 Unified image classification, object detection, and visual relationship detection.pdf:pdf},
month = {nov},
title = {{The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale}},
url = {http://arxiv.org/abs/1811.00982},
year = {2018}
}
@book{Kvam2007,
abstract = {This book presents modern nonparametric statistics from a practical point of view. It is primarily intended for use with engineers and scientists. While the book covers the necessary theorems and methods of rank tests in an applied fashion, the novelty lies in its emphasis on modern nonparametric methods in regression and curve fitting, bootstrap confidence intervals, splines, wavelets, empirical and nonparametric likelihood, and goodness of fit testing. MATLAB is the computing and programming system of choice throughout the book because of its special applicability for research analysis and simulation. Introduction -- Probability basics -- Statistics basics -- Bayesian statistics -- Order statistics -- Goodness of fit -- Rank tests -- Designed experiments -- Categorical data -- Estimating distribution functions -- Density estimation -- Beyond linear regression -- Curve fitting techniques -- Wavelets -- Bootstrap -- Em algorithm -- Statistical learning -- Nonparametric bayes.},
author = {Kvam, Paul H. and Vidakovic, Brani},
isbn = {9780470081471},
pages = {420},
publisher = {Wiley-Interscience},
title = {{Nonparametric statistics with applications to science and engineering}},
url = {https://www.wiley.com/en-us/Nonparametric+Statistics+with+Applications+to+Science+and+Engineering-p-9780470081471 http://zoe.bme.gatech.edu/{~}bv20/isye6404/Bank/npmarginal.pdf},
year = {2007}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms-for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several "visual Turing tests" probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
doi = {10.1126/science.aab3050},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tenenbaum - 2015 - Human-level concept learning through probabilistic program induction.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = {dec},
number = {6266},
pages = {1332--8},
pmid = {26659050},
publisher = {American Association for the Advancement of Science},
title = {{Human-level concept learning through probabilistic program induction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26659050},
volume = {350},
year = {2015}
}
@article{Lake2019,
abstract = {Three years ago, we released the Omniglot dataset for developing more human-like learning algorithms. Omniglot is a one-shot learning challenge, inspired by how people can learn a new concept from just one or a few examples. Along with the dataset, we proposed a suite of five challenge tasks and a computational model based on probabilistic program induction that addresses them. The computational model, although powerful, was not meant to be the final word on Omniglot; we hoped that the machine learning community would both build on our work and develop novel approaches to tackling the challenge. In the time since, we have been pleased to see the wide adoption of Omniglot and notable technical progress. There has been genuine progress on one-shot classification, but it has been difficult to measure since researchers have adopted different splits and training procedures that make the task easier. The other four tasks, while essential components of human conceptual understanding, have received considerably less attention. We review the progress so far and conclude that neural networks are still far from human-like concept learning on Omniglot, a challenge that requires performing all of the tasks with a single model. We also discuss new tasks to stimulate further progress.},
archivePrefix = {arXiv},
arxivId = {1902.03477},
author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
eprint = {1902.03477},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lake, Salakhutdinov, Tenenbaum - 2019 - The Omniglot Challenge A 3-Year Progress Report.pdf:pdf},
month = {feb},
title = {{The Omniglot Challenge: A 3-Year Progress Report}},
url = {http://arxiv.org/abs/1902.03477},
year = {2019}
}
@article{Lake2017,
abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
author = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
doi = {10.1017/S0140525X16001837},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lake et al. - 2017 - Building machines that learn and think like people.pdf:pdf},
journal = {Behavioral and Brain Sciences},
number = {e253},
title = {{Building machines that learn and think like people}},
url = {http://cims.nyu.edu/{~}brenden/ http://www.mit.edu/{~}tomeru/ http://web.mit.edu/cocosci/josh.html http://gershmanlab.webfactional.com/index.html},
volume = {40},
year = {2017}
}
@inproceedings{Lake2011,
abstract = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good fit to human perceptual data.},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B.},
booktitle = { Conference of the Cognitive Science Society (CogSci)},
title = {{One shot learning of simple visual concepts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.207.8634 https://cims.nyu.edu/{~}brenden/LakeEtAl2011CogSci.pdf},
year = {2011}
}
@article{Lansdell2018,
abstract = {In good old-fashioned artificial intelligence (GOFAI), humans specified systems that solved problems. Much of the recent progress in AI has come from replacing human insights by learning. However, learning itself is still usually built by humans -- specifically the choice that parameter updates should follow the gradient of a cost function. Yet, in analogy with GOFAI, there is no reason to believe that humans are particularly good at defining such learning systems: we may expect learning itself to be better if we learn it. Recent research in machine learning has started to realize the benefits of that strategy. We should thus expect this to be relevant for neuroscience: how could the correct learning rules be acquired? Indeed, cognitive science has long shown that humans learn-to-learn, which is potentially responsible for their impressive learning abilities. Here we discuss ideas across machine learning, neuroscience, and cognitive science that matter for the principle of learning-to-learn.},
archivePrefix = {arXiv},
arxivId = {1811.00231},
author = {Lansdell, Benjamin James and Kording, Konrad Paul},
eprint = {1811.00231},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lansdell, Kording - 2018 - Towards learning-to-learn.pdf:pdf},
month = {nov},
title = {{Towards learning-to-learn}},
url = {http://arxiv.org/abs/1811.00231},
year = {2018}
}
@article{Lecun1998,
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/},
volume = {86},
year = {1998}
}
@article{Lemke2015,
abstract = {Metalearning attracted considerable interest in the machine learning community in the last years. Yet, some disagreement remains on what does or what does not constitute a metalearning problem and in which contexts the term is used in. This survey aims at giving an all-encompassing overview of the research directions pursued under the umbrella of metalearning, reconciling different definitions given in scientific literature, listing the choices involved when designing a metalearning system and identifying some of the future research challenges in this domain.},
author = {Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
doi = {10.1007/s10462-013-9406-y},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lemke, Budka, Gabrys - 2015 - Metalearning a survey of trends and technologies.pdf:pdf},
journal = {Artificial Intelligence Review},
month = {jun},
number = {1},
pages = {117--130},
publisher = {Springer Netherlands},
title = {{Metalearning: a survey of trends and technologies}},
url = {http://link.springer.com/10.1007/s10462-013-9406-y},
volume = {44},
year = {2015}
}
@inproceedings{Lopez-Paz2017,
abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
author = {Lopez-Paz, David and Ranzato, Marc ' Aurelio},
booktitle = {NIPS},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Paz, Ranzato - 2017 - Gradient Episodic Memory for Continual Learning.pdf:pdf},
title = {{Gradient Episodic Memory for Continual Learning}},
url = {https://github.com/},
year = {2017}
}
@article{Mahajan2018,
abstract = {State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4{\%} (97.6{\%} top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.},
archivePrefix = {arXiv},
arxivId = {1805.00932},
author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
eprint = {1805.00932},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Mahajan et al. - 2018 - Exploring the Limits of Weakly Supervised Pretraining.pdf:pdf},
month = {may},
title = {{Exploring the Limits of Weakly Supervised Pretraining}},
url = {http://arxiv.org/abs/1805.00932},
year = {2018}
}
@techreport{Mareschal2002,
abstract = {Three-to 4-month-old infants show asymmetric exclusivity in the acquisition of cat and dog perceptual categories. The cat perceptual category excludes dog exemplars, but the dog perceptual category does not exclude cat exemplars. We describe a connectionist autoencoder model of perceptual categorization that shows the same asymmetries as infants. The model predicts the presence of asymmetric retroactive interference when infants acquire cat and dog categories sequentially. A subsequent experiment conducted with 3-to 4-month-olds verifies the predicted pattern of looking time behaviors. We argue that bottom-up, associative learning systems with distributed representations are appropriate for modeling the operation of short-term visual memory in early perceptual category learning.},
author = {Mareschal, Denis and Quinn, Paul C and French, Robert M},
booktitle = {Cognitive Science},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Mareschal, Quinn, French - 2002 - Asymmetric interference in 3-to 4-month-olds' sequential category learning.pdf:pdf},
keywords = {Asymmetric interference,Cat and dog categories,Sequential category learning},
pages = {377--389},
title = {{Asymmetric interference in 3-to 4-month-olds' sequential category learning}},
url = {https://onlinelibrary-wiley-com.ccl.idm.oclc.org/doi/pdf/10.1207/s15516709cog2603{\_}8},
volume = {26},
year = {2002}
}
@misc{Marr2017,
author = {Marr, Bernard},
booktitle = {Forbes},
title = {{The Amazing Ways Google Uses Deep Learning AI}},
url = {https://www.forbes.com/sites/bernardmarr/2017/08/08/the-amazing-ways-how-google-uses-deep-learning-ai/{\#}7d98e8e43204},
urldate = {2019-03-02},
year = {2017}
}
@book{Mars2011,
doi = {10.7551/mitpress/9780262016438.001.0001},
editor = {Mars, Rogier B. and Sallet, Jerome and Rushworth, Matthew F. S. and Yeung, Nick},
isbn = {9780262016438},
month = {oct},
publisher = {The MIT Press},
title = {{Neural Basis of Motivational and Cognitive Control}},
url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262016438.001.0001/upso-9780262016438},
year = {2011}
}
@article{McCloskey1989,
abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
author = {McCloskey, Michael and Cohen, Neal J.},
doi = {10.1016/S0079-7421(08)60536-8},
isbn = {9780125433242},
issn = {0079-7421},
journal = {Psychology of Learning and Motivation},
month = {jan},
pages = {109--165},
publisher = {Academic Press},
title = {{Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem}},
url = {https://www-sciencedirect-com.ccl.idm.oclc.org/science/article/pii/S0079742108605368},
volume = {24},
year = {1989}
}
@techreport{Mei2015,
abstract = {We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence "regions" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore gen-eralizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.04089v4},
author = {Mei, Hongyuan and Bansal, Mohit and Walter, Matthew R},
eprint = {arXiv:1506.04089v4},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Mei, Bansal, Walter - Unknown - Listen, Attend, and Walk Neural Mapping of Navigational Instructions to Action Sequences.pdf:pdf},
title = {{Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences}},
url = {https://arxiv.org/pdf/1506.04089.pdf},
year = {2015}
}
@techreport{Mishra2018,
abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.03141v3},
author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
eprint = {arXiv:1707.03141v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Mishra et al. - 2018 - A Simple Neural Attentive Meta-Learner.pdf:pdf},
title = {{A Simple Neural Attentive Meta-Learner}},
url = {https://arxiv.org/pdf/1707.03141.pdf},
year = {2018}
}
@article{Mozer2008,
author = {Mozer, Michael C. and Fan, Adrian},
doi = {10.1007/s11047-007-9041-0},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Mozer, Fan - 2008 - Top-Down modulation of neural responses in visual perception a computational exploration.pdf:pdf},
issn = {1567-7818},
journal = {Natural Computing},
month = {mar},
number = {1},
pages = {45--55},
publisher = {Springer Netherlands},
title = {{Top-Down modulation of neural responses in visual perception: a computational exploration}},
url = {http://link.springer.com/10.1007/s11047-007-9041-0},
volume = {7},
year = {2008}
}
@techreport{Munkhdalai2018,
abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neu-ral network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6{\%} accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00837v2},
author = {Munkhdalai, Tsendsuren and Yu, Hong},
eprint = {arXiv:1703.00837v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Munkhdalai, Yu - 2018 - Meta Networks.pdf:pdf},
title = {{Meta Networks}},
url = {https://arxiv.org/pdf/1703.00837.pdf},
year = {2018}
}
@techreport{Nayebi2018,
abstract = {Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, custom cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs explained the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.00053v1},
author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and Sussillo, David and Dicarlo, James J and Yamins, Daniel L K},
eprint = {arXiv:1807.00053v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Nayebi et al. - 2018 - Task-Driven Convolutional Recurrent Models of the Visual System.pdf:pdf},
title = {{Task-Driven Convolutional Recurrent Models of the Visual System}},
url = {https://arxiv.org/pdf/1807.00053.pdf},
year = {2018}
}
@incollection{Newell1980,
abstract = {Practice, and the performance improvement that it engenders, has long been a major topic in psychology. In this paper, both experimental and theoretical approaches are employed in an investigation of the mechanisms underlying this improvement. On the experimental side, it is argued that a single law, the power law of practice, adequately describes all of the practice data. On the theoretical side, a model of practice rooted in modern cognitive psychology, the chunking theory of learning, is formulated. The paper consists of (1) the presentation of a set of empirical practice curves; (2) mathematical investigations into the nature of power law functions; (3) evaluations of the ability of three different classes of functions to adequately model the empirical curves; (4) a discussion of the existing models of practice; and (5) a presentation of the chunking theory of learning.},
author = {Newell, Allen and Rosenbloom, Paul S},
booktitle = {Cognitive Skills and their Acquisition},
edition = {Hillsdale,},
editor = {Anderson, J. R.},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Newell, Rosenbloom - 1980 - Mechanisms of Skill Acquisition and the Law of Practice.pdf:pdf},
publisher = {Erlbaum},
title = {{Mechanisms of Skill Acquisition and the Law of Practice.}},
url = {https://apps.dtic.mil/docs/citations/ADA096527},
year = {1980}
}
@article{Nguyen2018,
abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
archivePrefix = {arXiv},
arxivId = {1710.10628},
author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
eprint = {1710.10628},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2017 - Variational Continual Learning.pdf:pdf},
journal = {ICLR},
month = {oct},
title = {{Variational Continual Learning}},
url = {http://arxiv.org/abs/1710.10628},
year = {2018}
}
@techreport{Nichol,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.02999v2},
author = {Nichol, Alex and Achiam, Joshua and Openai, John Schulman},
eprint = {arXiv:1803.02999v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Nichol, Achiam, Openai - Unknown - On First-Order Meta-Learning Algorithms.pdf:pdf},
title = {{On First-Order Meta-Learning Algorithms}},
url = {https://arxiv.org/pdf/1803.02999.pdf},
year = {2018}
}
@article{Olah2017,
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
issn = {2476-0757},
journal = {Distill},
month = {nov},
number = {11},
pages = {e7},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization},
volume = {2},
year = {2017}
}
@article{Parisi2019,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/J.NEUNET.2019.01.012},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Parisi et al. - 2019 - Continual lifelong learning with neural networks A review.pdf:pdf},
journal = {Neural Networks},
month = {may},
pages = {54--71},
publisher = {Pergamon},
title = {{Continual lifelong learning with neural networks: A review}},
url = {https://www-sciencedirect-com.ccl.idm.oclc.org/science/article/pii/S0893608019300231 https://arxiv.org/pdf/1802.07569.pdf},
volume = {113},
year = {2019}
}
@inproceedings{Perez2017,
abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
archivePrefix = {arXiv},
arxivId = {1709.07871},
author = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
booktitle = {AAAI'18},
eprint = {1709.07871},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Perez et al. - 2017 - FiLM Visual Reasoning with a General Conditioning Layer.pdf:pdf},
month = {sep},
title = {{FiLM: Visual Reasoning with a General Conditioning Layer}},
url = {http://arxiv.org/abs/1709.07871},
year = {2018}
}
@inproceedings{Pratt1993,
abstract = {Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm. called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly.},
author = {Pratt, L Y},
booktitle = {NIPS},
file = {::},
title = {{Discriminability-Based Transfer between Neural Networks}},
url = {http://papers.nips.cc/paper/641-discriminability-based-transfer-between-neural-networks.pdf},
year = {1993}
}
@techreport{Raposo2017,
abstract = {Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs)-a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.},
archivePrefix = {arXiv},
arxivId = {1702.05068v1},
author = {Raposo, D and Santoro, A and Barrett, D G T and Pascanu, R and Lillicrap, T and Battaglia, P},
eprint = {1702.05068v1},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Raposo et al. - 2017 - Discovering objects and their relations from entangled scene representations.pdf:pdf},
institution = {ICLR},
title = {{Discovering objects and their relations from entangled scene representations}},
url = {https://arxiv.org/pdf/1702.05068.pdf},
year = {2017}
}
@techreport{Ritter2017,
abstract = {Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more inter-pretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cog-nitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models , and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
archivePrefix = {arXiv},
arxivId = {1706.08606v2},
author = {Ritter, Samuel and Barrett, David G T and Santoro, Adam and Botvinick, Matt M},
eprint = {1706.08606v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Ritter et al. - 2017 - Cognitive Psychology for Deep Neural Networks A Shape Bias Case Study(2).pdf:pdf},
title = {{Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study}},
url = {https://arxiv.org/pdf/1706.08606.pdf},
year = {2017}
}
@inproceedings{Ritter2018a,
abstract = {Recent research has placed episodic reinforcement learning (RL) alongside model-free and model-based RL on the list of processes centrally involved in human reward-based learning. In the present work, we extend the unified account of model-free and model-based RL developed by Wang et al. (2018) to further integrate episodic learning. In this account, a generic model-free "meta-learner" learns to deploy and coordinate among all of these learning algorithms. The meta-learner learns through brief encounters with many novel tasks, so that it learns to learn about new tasks. We show that when equipped with an episodic memory system inspired by theories of reinstatement and gating, the meta-learner learns to use the episodic and model-based learning algorithms observed in humans in a task designed to dissociate among the influences of various learning strategies. We discuss implications and predictions of the model.},
author = {Ritter, Samuel and Wang, Jane X and Kurth-Nelson, Zeb and Botvinick, M},
booktitle = {CogSci},
doi = {10.1101/360537},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Ritter et al. - 2018 - Episodic Control as Meta-Reinforcement Learning.pdf:pdf},
title = {{Episodic Control as Meta-Reinforcement Learning}},
url = {http://dx.doi.org/10.1101/360537},
year = {2018}
}
@techreport{Ritter2018,
abstract = {Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoc-cur-as they do in natural environments-meta-learning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differ-entiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccur-ring tasks, ranging from bandits to navigation and stochastic sequential decision problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.09692v2},
author = {Ritter, Samuel and Wang, Jane X and Kurth-Nelson, Zeb and Jayakumar, Siddhant M and Blundell, Charles and Pascanu, Razvan and Botvinick, Matthew},
eprint = {arXiv:1805.09692v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Ritter et al. - 2018 - Been There, Done That Meta-Learning with Episodic Recall.pdf:pdf},
title = {{Been There, Done That: Meta-Learning with Episodic Recall}},
url = {https://arxiv.org/pdf/1805.09692.pdf},
year = {2018}
}
@misc{Rossant2016,
author = {Rossant, Cyrille},
title = {{Should you use HDF5?}},
url = {https://cyrille.rossant.net/should-you-use-hdf5/},
urldate = {2019-02-28},
year = {2016}
}
@article{Russo1998,
abstract = {Memory for repeated items improves when presentations are spaced during study. In Experiment 1A, words were repeated either immediately or after 6 intervening items. Intentional learning occurred under either focused or divided attention. Retention was tested by either free recall or yes-no recognition. Divided attention did not affect the influence of spacing in free recall, whereas it removed the spacing effect in recognition. In Experiment 1B, recognition memory was tested after incidental semantic study of words performed under either focused or divided attention. An equivalent spacing effect occurred in both attentional conditions. In Experiments 2 and 3, recognition memory for unfamiliar faces was assessed. A reliable spacing effect was found under both intentional learning and incidental structural study. These data are, collectively, incompatible with current theories of spacing effects. A theoretical proposal to account for these new findings is outlined.},
author = {Russo, R and Parkin, A J and Taylor, S R and Wilks, J},
issn = {0278-7393},
journal = {Journal of experimental psychology. Learning, memory, and cognition},
month = {jan},
number = {1},
pages = {161--72},
pmid = {9438957},
title = {{Revising current two-process accounts of spacing effects in memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9438957},
volume = {24},
year = {1998}
}
@techreport{Santoro2016,
abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning ." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architec-tures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information , and hence can potentially obviate the down-sides of conventional models. Here, we demonstrate the ability of a memory-augmented neu-ral network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Santoro et al. - 2016 - Meta-Learning with Memory-Augmented Neural Networks Google DeepMind(2).pdf:pdf},
title = {{Meta-Learning with Memory-Augmented Neural Networks}},
url = {http://proceedings.mlr.press/v48/santoro16.pdf},
year = {2016}
}
@techreport{Santoro2018,
abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected-i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module-a Relational Memory Core (RMC)-which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.},
archivePrefix = {arXiv},
arxivId = {1806.01822v2},
author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and {Rae $\alpha$$\beta$}, Jack and {Chrzanowski $\alpha$}, Mike and {Weber $\alpha$}, Th{\'{e}}ophane and {Wierstra $\alpha$}, Daan and {Vinyals $\alpha$}, Oriol and {Pascanu $\alpha$}, Razvan and {Lillicrap $\alpha$$\beta$}, Timothy},
eprint = {1806.01822v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Santoro et al. - Unknown - Relational recurrent neural networks.pdf:pdf},
title = {{Relational recurrent neural networks}},
url = {https://arxiv.org/pdf/1806.01822.pdf},
year = {2018}
}
@article{Santoro,
abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
author = {Santoro, Adam and Raposo, David and Barrett, David G T and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy and London, Deepmind},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
title = {{A simple neural network module for relational reasoning}},
url = {https://arxiv.org/pdf/1706.01427.pdf},
year = {2017}
}
@phdthesis{Schmidhuber1987,
author = {Schmidhuber, Jurgen},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 1987 - Evolutionary Principles in Self-Referential Learning.pdf:pdf},
school = {TU Munich},
title = {{Evolutionary Principles in Self-Referential Learning}},
url = {http://people.idsia.ch/{~}juergen/diploma1987ocr.pdf},
year = {1987}
}
@book{Sejnowski2018,
abstract = {How deep learning--from Google Translate to driverless cars to personal cognitive assistants--is changing our lives and transforming every sector of the economy. The deep learning revolution has brought us driverless cars, the greatly improved Google Translate, fluent conversations with Siri and Alexa, and enormous profits from automated trading on the New York Stock Exchange. Deep learning networks can play poker better than professional poker players and defeat a world champion at Go. In this book, Terry Sejnowski explains how deep learning went from being an arcane academic field to a disruptive technology in the information economy. Sejnowski played an important role in the founding of deep learning, as one of a small group of researchers in the 1980s who challenged the prevailing logic-and-symbol based version of AI. The new version of AI Sejnowski and others developed, which became deep learning, is fueled instead by data. Deep networks learn from data in the same way that babies experience the world, starting with fresh eyes and gradually acquiring the skills needed to navigate novel environments. Learning algorithms extract information from raw data; information can be used to create knowledge; knowledge underlies understanding; understanding leads to wisdom. Someday a driverless car will know the road better than you do and drive with more skill; a deep learning network will diagnose your illness; a personal cognitive assistant will augment your puny human brain. It took nature many millions of years to evolve human intelligence; AI is on a trajectory measured in decades. Sejnowski prepares us for a deep learning future.},
author = {Sejnowski, Terrence J.},
isbn = {9780262038034},
pages = {352},
publisher = {The MIT Press},
title = {{The deep learning revolution}},
url = {https://mitpress.mit.edu/books/deep-learning-revolution},
year = {2018}
}
@inproceedings{Shadmehr1994,
author = {Shadmehr, Reza and Brashers-Krug, Tom and Mussa-Ivaldi, Ferdinando},
booktitle = {NIPS},
title = {{Interference in learning internal models of inverse dynamics in humans}},
url = {https://dl-acm-org.ccl.idm.oclc.org/citation.cfm?id=2998826},
year = {1994}
}
@techreport{Shyam2017,
abstract = {Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a dynamic representation space and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5{\%}. This represents the first superhuman result achieved for this task with a generic model that uses only pixel information.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.00767v3},
author = {Shyam, Pranav and Gupta, Shubham and Dukkipati, Ambedkar},
eprint = {arXiv:1703.00767v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Shyam, Gupta, Dukkipati - 2017 - Attentive Recurrent Comparators.pdf:pdf},
title = {{Attentive Recurrent Comparators}},
url = {https://arxiv.org/pdf/1703.00767.pdf},
year = {2017}
}
@article{Silver2018,
abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
doi = {10.1126/science.aar6404},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - 2018 - A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = {dec},
number = {6419},
pages = {1140--1144},
pmid = {30523106},
publisher = {American Association for the Advancement of Science},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30523106},
volume = {362},
year = {2018}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {::},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Snell2017,
abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
author = {Snell, Jake and Twitter, Kevin Swersky and Zemel, Richard S},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Snell, Twitter, Zemel - Unknown - Prototypical Networks for Few-shot Learning.pdf:pdf},
title = {{Prototypical Networks for Few-shot Learning}},
url = {https://arxiv.org/pdf/1703.05175.pdf},
year = {2017}
}
@article{Spitzer1939,
author = {Spitzer, H. F.},
doi = {10.1037/h0063404},
issn = {0022-0663},
journal = {Journal of Educational Psychology},
number = {9},
pages = {641--656},
title = {{Studies in retention.}},
url = {http://content.apa.org/journals/edu/30/9/641},
volume = {30},
year = {1939}
}
@article{Spoerer2017,
abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and non-human primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, digit clutter (where multiple target digits occlude one another) and digit debris (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognizing objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognize objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
doi = {10.3389/fpsyg.2017.01551},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {convolutional neural network,object recognition,occlusion,recurrent neural network,top-down processing},
month = {sep},
pages = {1551},
pmid = {28955272},
title = {{Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28955272 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5600938 https://www.frontiersin.org/article/10.3389/fpsyg.2017.01551/full},
volume = {8},
year = {2017}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting(2).pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@inproceedings{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approaching human-level performance.},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.220},
isbn = {978-1-4799-5118-5},
month = {jun},
pages = {1701--1708},
publisher = {IEEE},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909616},
year = {2014}
}
@misc{TheHDFGroup1997,
author = {{The HDF Group}},
title = {{Hierarchical Data Format, version 5}},
url = {https://www.hdfgroup.org/solutions/hdf5/},
urldate = {2019-02-28},
year = {1997}
}
@article{Thorpe1996,
abstract = {How long does it take for the human visual system to process a complex natural image? Subjectively, recognition of familiar objects and scenes appears to be virtually instantaneous, but measuring this processing time experimentally has proved difficult. Behavioural measures such as reaction times can be used, but these include not only visual processing but also the time required for response execution. However, event-related potentials (ERPs) can sometimes reveal signs of neural processing well before the motor output. Here we use a go/no-go categorization task in which subjects have to decide whether a previously unseen photograph, flashed on for just 20 ms, contains an animal. ERP analysis revealed a frontal negativity specific to no-go trials that develops roughly 150 ms after stimulus onset. We conclude that the visual processing needed to perform this highly demanding task can be achieved in under 150 ms.},
author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
doi = {10.1038/381520a0},
issn = {0028-0836},
journal = {Nature},
month = {jun},
number = {6582},
pages = {520--522},
pmid = {8632824},
title = {{Speed of processing in the human visual system}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8632824 http://www.nature.com/articles/381520a0},
volume = {381},
year = {1996}
}
@techreport{Thrun1996,
abstract = {This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.},
author = {Thrun, Sebastian},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Thrun - 1996 - Is Learning The n-th Thing Any Easier Than Learning The First.pdf:pdf},
title = {{Is Learning The n-th Thing Any Easier Than Learning The First?}},
url = {https://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
year = {1996}
}
@article{Thrun1995,
abstract = {Learning provides a useful tool for the automatic design of autonomous robots. Recent research on learning robot control has predominantly focused on learning single tasks that were studied in isolation. If robots encounter a multitude of control learning tasks over their entire lifetime there is an opportunity to transfer knowledge between them. In order to do so, robots may learn the invariants and the regularities of the individual tasks and environments. This task-independent knowledge can be employed to bias generalization when learning control, which reduces the need for real-world experimentation. We argue that knowledge transfer is essential if robots are to learn control with moderate learning times in complex scenarios. Two approaches to lifelong robot learning which both capture invariant knowledge about the robot and its environments are presented. Both approaches have been evaluated using a HERO-2000 mobile robot. Learning tasks included navigation in unknown indoor environments and a simple find-and-fetch task.},
author = {Thrun, Sebastian and Mitchell, Tom M.},
doi = {10.1016/0921-8890(95)00004-Y},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
month = {jul},
number = {1-2},
pages = {25--46},
publisher = {North-Holland},
title = {{Lifelong robot learning}},
url = {https://www-sciencedirect-com.ccl.idm.oclc.org/science/article/pii/092188909500004Y},
volume = {15},
year = {1995}
}
@inproceedings{Tompson2015,
abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
archivePrefix = {arXiv},
arxivId = {1411.4280},
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
booktitle = {CVPR},
eprint = {1411.4280},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional Networks.pdf:pdf},
title = {{Efficient Object Localization Using Convolutional Networks}},
url = {https://arxiv.org/abs/1411.4280},
year = {2015}
}
@inproceedings{Toneva2019,
abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a `forgetting event' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
archivePrefix = {arXiv},
arxivId = {1812.05159},
author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
booktitle = {ICLR},
eprint = {1812.05159},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Toneva et al. - 2019 - An Empirical Study of Example Forgetting during Deep Neural Network Learning.pdf:pdf},
title = {{An Empirical Study of Example Forgetting during Deep Neural Network Learning}},
url = {http://arxiv.org/abs/1812.05159},
year = {2019}
}
@techreport{Triantafillou2017,
abstract = {Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular , we view each batch point as a 'query' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.},
author = {Triantafillou, Eleni and Zemel, Richard and Urtasun, Raquel},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Triantafillou, Zemel, Urtasun - 2017 - Few-Shot Learning Through an Information Retrieval Lens.pdf:pdf},
title = {{Few-Shot Learning Through an Information Retrieval Lens}},
url = {https://papers.nips.cc/paper/6820-few-shot-learning-through-an-information-retrieval-lens.pdf},
year = {2017}
}
@techreport{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v5},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {arXiv:1706.03762v5},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/pdf/1706.03762.pdf},
year = {2017}
}
@article{Vatterott2018,
author = {Vatterott, Daniel B. and Mozer, Michael C. and Vecera, Shaun P.},
doi = {10.3758/s13414-017-1465-8},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Vatterott, Mozer, Vecera - 2018 - Rejecting salient distractors Generalization from experience.pdf:pdf},
issn = {1943-3921},
journal = {Attention, Perception, {\&} Psychophysics},
month = {feb},
number = {2},
pages = {485--499},
publisher = {Springer US},
title = {{Rejecting salient distractors: Generalization from experience}},
url = {http://link.springer.com/10.3758/s13414-017-1465-8},
volume = {80},
year = {2018}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1606.04080},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf:pdf},
month = {jun},
title = {{Matching Networks for One Shot Learning}},
url = {http://arxiv.org/abs/1606.04080},
year = {2016}
}
@article{Waldron2001,
abstract = {Participants learned simple and complex category structures under typical single-task conditions and when performing a simultaneous numerical Stroop task. In the simple categorization tasks, each set of contrasting categories was separated by a unidimensional explicit rule, whereas the complex tasks required integrating information from three stimulus dimensions and resulted in implicit rules that were difficult to verbalize. The concurrent Stroop task dramatically impaired learning of the simple explicit rules, but did not significantly delay learning of the complex implicit rules. These results support the hypothesis that category learning is mediated by multiple learning systems.},
author = {Waldron, Elliott M. and Ashby, F. Gregory},
doi = {10.3758/BF03196154},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Waldron, Ashby - 2001 - The effects of concurrent task interference on category learning Evidence for multiple category learning systems.pdf:pdf},
journal = {Psychonomic Bulletin {\&} Review},
month = {mar},
number = {1},
pages = {168--176},
publisher = {Springer-Verlag},
title = {{The effects of concurrent task interference on category learning: Evidence for multiple category learning systems}},
url = {http://www.springerlink.com/index/10.3758/BF03196154 https://link-springer-com.ccl.idm.oclc.org/content/pdf/10.3758/BF03196154.pdf},
volume = {8},
year = {2001}
}
@techreport{Wang2016,
abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.05763v3},
author = {Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, D and Soyer, H and Leibo, J Z and Munos, R and Blundell, C and Kumaran, D and Botvinick, M},
eprint = {arXiv:1611.05763v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2016 - Learning to Reinforcement Learn.pdf:pdf},
title = {{Learning to Reinforcement Learn}},
url = {https://arxiv.org/pdf/1611.05763v3.pdf},
year = {2016}
}
@techreport{Wu2016,
abstract = {We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible. It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA [23] and VQA [1] and show that it produces the best reported results in both cases.},
archivePrefix = {arXiv},
arxivId = {1511.06973v2},
author = {Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and {Van Den Hengel}, Anton},
eprint = {1511.06973v2},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Ask Me Anything Free-form Visual Question Answering Based on Knowledge from External Sources.pdf:pdf},
isbn = {1511.06973v2},
title = {{Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources}},
url = {https://arxiv.org/pdf/1511.06973.pdf},
year = {2016}
}
@techreport{Wu2016a,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {https://ai.google/research/pubs/pub45610},
year = {2016}
}
@inproceedings{Xiang2017,
abstract = {In CNN-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state-of-the-art performance on both detection and pose estimation on commonly used benchmarks.},
archivePrefix = {arXiv},
arxivId = {1604.04693},
author = {Xiang, Yu and Choi, Wongun and Lin, Yuanqing and Savarese, Silvio},
booktitle = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
doi = {10.1109/WACV.2017.108},
eprint = {1604.04693},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Santoro et al. - 2017 - A simple neural network module for relational reasoning.pdf:pdf},
isbn = {9781509048229},
issn = {21607516},
number = {June},
pages = {924--933},
pmid = {189384},
title = {{Subcategory-Aware convolutional neural networks for object proposals {\&} detection}},
url = {https://arxiv.org/pdf/1706.01427.pdf},
year = {2017}
}
@techreport{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03044v3},
author = {Xu, Kelvin and Ba, Jimmy Lei and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
eprint = {arXiv:1502.03044v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {https://arxiv.org/pdf/1502.03044.pdf},
year = {2015}
}
@article{Yamins2016,
abstract = {Recent computational neuroscience developments have used deep neural networks to model neural responses in higher visual areas. This Perspective describes key algorithmic underpinnings in computer vision and artificial intelligence that have contributed to this progress and outlines how deep networks could drive future improvements in understanding sensory cortical processing.},
author = {Yamins, Daniel L K and DiCarlo, James J},
doi = {10.1038/nn.4244},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Yamins, DiCarlo - 2016 - Using goal-driven deep learning models to understand sensory cortex.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Computational neuroscience,Object vision},
month = {mar},
number = {3},
pages = {356--365},
publisher = {Nature Publishing Group},
title = {{Using goal-driven deep learning models to understand sensory cortex}},
url = {http://www.nature.com/articles/nn.4244},
volume = {19},
year = {2016}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
booktitle = {ECCV},
eprint = {1311.2901},
file = {::},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2014}
}
@inproceedings{Zenke2017,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
archivePrefix = {arXiv},
arxivId = {1703.04200v3},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {ICML},
eprint = {1703.04200v3},
file = {:Users/guydavidson/Library/Application Support/Mendeley Desktop/Downloaded/Zenke, Poole, Ganguli - 2017 - Continual Learning Through Synaptic Intelligence.pdf:pdf},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {https://arxiv.org/pdf/1703.04200.pdf},
year = {2017}
}
