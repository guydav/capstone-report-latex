\section{Neural Networks}
\begin{itemize}
\item \textbf{Activation:\label{tg:activation}} the activation of a neural network is its response after a particular layer to an input. If we consider the simple case of a fully connected layer (see \hyperref[tg:mlp]{multilayer perceptron} below), which multiplies the input by a weight matrix, adds a bias vector, and applies a nonlinearity (or activation function), the activation is the output of this set of operations before it is fed as the input to the next layer.

\item \textbf{Backpropagation:\label{tg:backprop}} a learning algorithm allowing to update the \hyperref[tg:weights]{weights} (see above) of a neural network using a repeated application of the chain rule. We begin from the loss function, and compute its gradient with respect to all output units. For each such output unit, we can them compute the gradient with respect to all of its inputs by applying the chain rule, and repeatedly apply this method for every layer in the network. This \href{https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e}{\emph{Medium post}} articulates a longer example, and this \href{https://brilliant.org/wiki/backpropagation/}{\emph{Brilliant.org entry}} provides a more rigorous explanation.

\item \textbf{Convolutional neural network (CNN):\label{tg:cnn}} a neural network architecture designed primarily for image processing by embodying a spatial invariance bias: a face is still a face, regardless of what part of an image it might appear in. CNNs operate by applying a group of filters at all spatial locations on the input (i.e., convolving the input with the filters). The \href{http://cs231n.github.io/convolutional-networks/\#conv}{\emph{diagram under ``Convolution Demo''}} provides a useful demonstration. The  \href{http://cs231n.github.io/convolutional-networks/\#overview}{\emph{course notes}} for Stanford's CS231n provide additional details. A few other terms used to describe convolutional neural networks:

    \begin{itemize}
    \item \textbf{Filter (kernel):\label{tg:filter}} the feature extractors at each convolutional layer, which are convolved with the input data to produce an output map. Specified with a their size, where 3x3 is fairly standard, but some architectures also employ 1x1, 5x5, and other, more exotic choices. \href{http://cs231n.github.io/understanding-cnn/}{\emph{A demonstration of different techniques to visualize filters}}.
 
    \item \textbf{Stride:\label{tg:stride}} how many units (pixels, in an image) to move between different locations where the filters are applied on the image. Strides of 1 and 2 are common, and longer strides can make sense in particular contexts.
 
    \item \textbf{Padding:\label{tg:padding}} how many additional units to add around the input to a layer. Usually used to allow the filters to be also centered at units on the edge of the input itself.

    \item \emph{For instance, a common choice is to have 3x3 filters, with a stride of 1 and padding of 1. These choices mean the output for this convolutional layer would have the same height and weight that the input has.}
 
    \item \textbf{Pooling:\label{tg:pooling}} pooling layers usually follow convolutional ones and provide a mechanism to reduce size between layers. Pooling layers are parameterized by a size (often 2x2) and a function (usually either max or mean). A pooling layer is applied on non-overlapping chunks of the input unit, reducing each chunk to a single value using the function. See \href{http://deeplearning.stanford.edu/tutorial/supervised/Pooling/}{\emph{this tutorial}} for a longer description.
 
    \item \textbf{Batch normalization (BN):\label{tg:batch-norm}} another construct that tends to appear with convolutional layers (though applies to other layer types as well), batch normalization attempts to account for the fact that each batch might not represent the population well, and normalize the outputs of the layer accordingly. Intuitively, rather than work with the raw outputs, we keep track of the mean and standard deviation of each output, and normalize accordingly.  \href{https://www.alexirpan.com/public/perils-batch-norm/batch_norm_appendix.html}{\emph{This post}} provides an elaboration.
    \end{itemize}
    
\item \textbf{Forward pass (and backward pass):\label{tg:forward-backward-pass}} the processing in a neural network is generally considered to happen in two stages. The forward part is the prediction stage, in which a network processes an input and produces an output. During training, the \hyperref[tg:loss]{loss function} (see above) is them computed on the output and the ground truth value. The backward pass then updates the network using an \hyperref[tg:optimizer]{optimizer} (see below), hopefully resulting in \hyperref[tg:weights]{weights} (see above) better adapted to the task. During testing we only perform a forward pass.

\item \textbf{Fully connected layer/network (FC):\label{tg:fc}} see: \hyperref[tg:mlp]{multilayer perceptron}.

\item \textbf{Layer:\label{tg:layer}} a unit of organization in a neural network, which models a collection of neurons or nodes operating jointly at some depth of the network. These units (usually) all receive the same input and send their outputs to the same next layer, if one exists. The `deep' in deep learning reflects the stacking of higher numbers of layers, which has shown over the last seven years to achieve excellent results on a variety of problems.

\item \textbf{Multilayer perceptron (MLP):\label{tg:mlp}} a basic neural network architecture, in which all units in each layer project into all units in the following layer. Usually described as an input layer, one or more intermediate, hidden layers, and an output layer. Each layer is usually implemented as a matrix multiplication, followed by adding a bias vector, and threshold using an activation function (see \hyperref[tg:nonlinearity]]{nonlinearity} below) The \href{http://cs231n.github.io/neural-networks-1/\#layers}{\emph{course notes}} for Stanford's CS231n provide additional details.

\item \textbf{Nonlinearities (activation functions):\label{tg:nonlinearity}} in order to allow neural networks to learn complicated functions, we must introduce some nonlinearity into the process, as otherwise, a series of linear transformations remains a linear transformation. Two common choices are the sigmoid function, $\sigma(x) = \frac{1}{1 + \exp(-x)}$, which squashes the input into the unit interval, and the rectified linear unit (ReLU), $f(x) = max(0, x)$, which flattens negative inputs. See additional details in the \href{http://cs231n.github.io/neural-networks-1/\#actfun}{\emph{course notes}} for Stanford's CS231n.

\item \textbf{Optimizer:\label{tg:optimizer}} while most neural networks use the backpropagation algorithm to arrive at a \hyperref[tg:gradient]{gradient} for each weight, there are various optimization algorithms that offer different methods to update the weight using the gradient. \hyperref[tg:sgd]{Gradient descent} (see above) offers a baseline method, but many variations exist, involving a momentum term that accumulates recent gradients, adaptive learning rates, and many other modifications. This \href{http://ruder.io/optimizing-gradient-descent/}{\emph{post}} provides an \emph{extremely} thorough survey of many optimizers.

    \begin{itemize} 
    \item \textbf{Adam (adaptive moment estimation):\label{tg:adam}} a notably popular optimizer, Adam utilizes both a momentum term and adaptive learning rates in order to accelerate the training of neural networks over (vanilla) stochastic gradient descent. This \href{http://ruder.io/optimizing-gradient-descent/index.html\#adam}{\emph{section}} in the post above offers a longer elaboration.
    \item \textbf{Learning rate:} most optimizers use a learning rate to scale down the \hyperref[tg:gradient]{gradients} (see above), taking smaller step in the direction that best minimizes the \hyperref[tg:loss]{loss function} (see above). As with many hyperparameters, learning rate selection is more art than science, unless hyperparameter optimization is performed. If the learning rate is set too low, training will proceed slower than it optimally could. However, if the learning rate is set too high, training may fail, as the network will repeatedly bounce between bad solutions. This \href{https://www.jeremyjordan.me/nn-learning-rate/}{\emph{blog post}} provides a simple illustration of these phenomena and discusses different approaches to set it.
    \end{itemize}
 
\item \textbf{Recurrent neural networks (RNNs):\label{tg:rnn}} a class of neural network models that include a component from the previous forward pass that feeds into the current one. Due to this nature, these models are particularly popular with sequential data, notably natural language, but also with time series and other data with structure between consecutive inputs. This \href{https://skymind.ai/wiki/lstm}{\emph{tutorial post}} introduces RNNs in additional depth, and describes a popular variant, the long-short term memory (LSTM) model.

\item \textbf{Regularization:\label{tg:regularization}} a set of techniques designed to prevent a network from overfitting. While standard machine learning regularization techniques, such as penalizing the L1 or L2 norms of the weights, also work for deep neural networks, several other specialized approaches were developed as well. The course notes for Stanford's CS231n review a few approaches: \href{http://cs231n.github.io/neural-networks-2/\#reg}{\emph{http://cs231n.github.io/neural-networks-2/\#reg}}

    \begin{itemize} 
    \item \textbf{Dropout:\label{tg:dropout}} A form of regularization that attempts to help the network generalize. During training, units are kept active with a fixed probability $p$, and otherwise set to zero. This forces the network to learn to represent inputs well with only a subset of the units, ideally increasing robustness.
 
    \item \textbf{Spatial dropout:\label{tg:spatial-dropout}} A variant of dropout for convolutional neural networks. Due to the spatial structure and pooling layers, dropping out units at random isn't as sensible as it is with a fully-connected layer. Instead, spatial dropout turns off entire filter maps, attempting to force the network into some redundancy between the filters (just as regular dropout does with individual units).
 
    \item \textbf{Weight decay:\label{tg:weight-decay}} another regularization approach, which after every round of training, adds a penalty to the loss proportional to the current value of the weight, which can be also viewed as multiplying it by a constant slightly below one. This \href{https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate}{\emph{StackExchange}} answer provides additional detail.
    \end{itemize}
\end{itemize}