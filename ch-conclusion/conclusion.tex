We presented an original dataset and a visual question answering-inspired task design, which together allowed us to create one novel benchmark and suggest another one. The sequential benchmark we implemented allows examining how a model's learning performance changes with the number of tasks it already learned and with how many times it relearned each particular task. To measure how learning scales, we examined the number of examples a model required to learn a task to a 95\% accuracy criterion. To quantify catastrophic interference, we tested the accuracy immediately after introducing a new task to the model. We first examined how a baseline model, comprised of a simple convolutional neural network followed by a fully connected neural network, fared on this sequential benchmark. We then compared this baseline model to a set of models performing query-modulated convolutional processing, allowing the query to adjust the visual processing of the image presented, and analyze how performing this modulation at different layers of the convolutional model impacts performance.

We discovered that the baseline model, which is the minimal model we expected to be able to perform this task, shows positive scaling behavior: the more tasks it learns, the faster it tends to acquire the next task presented. This result was not intuitive to us -- we interpret it as suggesting that gradient-based optimization, coupled with this sequential presentation of tasks, allows the model to iteratively improve the quality of the weights it learns, putting it in a better starting position to acquire the next task. We consider this as an example of meta-learning, of learning how to improve the learning process for subsequent tasks. This result is different than previous models of gradient-based meta-learning, e.g., \textcite{Hochreiter2001}, which uses standard gradient descent to learn, but requires a recurrent neural network controller which functions as the meta-learning system. This work demonstrates meta-learning characteristics using a fairly naive model, with no specialized architecture. We note that model-agnostic meta-learning (MAML; \cite{Finn2017}) explicitly optimizes for weights that would be beneficial in learning multiple tasks, and we predict that an explicit meta-learning algorithm would perform better than our naive model. We find it intriguing that a model with no specific meta-learning features, neither in the architecture nor in the optimization procedure, demonstrates this type of positive scaling with additional training.

Conversely, when we examined the degree of catastrophic forgetting exhibited by this model, we discovered that the baseline model shows higher degrees of forgetting as it learns additional tasks. In the first episode introducing a new task, accuracy on the first few tasks the model learns dropped to around 75\%, while the accuracy on tasks introduced later falls to about 55\%. As noted above, the tasks introduced later also required fewer training examples to reach this criterion in first place. We interpret this behavior as an example of a stability-flexibility tradeoff (see \cite{Hermundstad2011}), in which over the course of the benchmark, the model acquires representations that are more flexible (and allow for faster learning) but are also less stable (and therefore show more rapid losses). It would be interesting to attempt to visualize these representations as a model learns additional tasks and see if we can find evidence for this stability-flexibility notion in the data. While tasks introduced later show higher forgetting, this effect tapers off after a task was trained on for a few times, showing a consistently lower degree of forgetting with additional training. In other words, what helps mitigate catastrophic interference is not domain expertise, which is acquired by learning additional tasks, but further practice with the same task. 

Our results on the control (sequential, heterogeneous dimensions) condition suggest that the interference for repeated learning within a dimension, on the baseline model, is higher than any benefits conferred by accruing knowledge within the domain of a particular dimension. An alternative interpretation might be that training on multiple dimensions, rather than a single one, encourages the model to retain additional types of visual information, which compete less, conferring an advantage rather than a disadvantage. The transfer occurring is therefore not dimension-specific, but rather a general improvement in the model's visual processing capacity. It is possible that if the heterogeneous tasks were more diverse, either including substantially different stimuli, or a variety of different computer vision paradigms (such as classification, semantic segmentation, and object detection), interference between tasks in the heterogeneous condition would be higher than in the homogeneous condition.

The query-modulated models show an improvement in some of the learning metrics examined, but not others. Query-modulation on all levels helps to learn the first few tasks faster and does not show the highly anomalous behavior in the second repetition of the first task, suggesting perhaps that the representations learned are stabler. However, both later times trained on the first few tasks, as well as all times trained on the tasks introduced later, show no such effect, and in some cases, the effect even reverses. This finding may imply that the representations the baseline, non-query modulated model acquires later in training are similar to the ones the query-modulated models learned from the beginning, another question we could investigate by visualizing the representations. While the query-modulated models all show the power law of practice effect on each task, as evidenced by the linear behavior in the log-log plot, they show weaker evidence of positive scaling within the domain for tasks introduced later. 

In catastrophic forgetting, the query-modulated models all do substantially better. All query-modulated models show higher accuracies in the first episodes introducing a new task, with the most robust effect in the second and third times the model learned each task, and for the earlier tasks a model learns. If we ascribe to the stability-flexibility tradeoff interpretation described above, query-modulation enables learning equally flexible representations that are more stable than those acquired without query-based modulation. In some sense, it is sensible that query-modulation of the visual processing should allow for this increased stability, as the model might achieve the required flexibility by changing the weights associated with the query, rather than with the convolutional filters; but we do not find these results obvious. 

The different performance profiles on the two aspects examined in this benchmark, the scaling behavior of the learning performance and the proclivity to catastrophic forgetting, reinforces the benefits of measuring and analyzing both. To paint a full picture of how well a particular model architecture or optimization procedure is capable of learning, we see demonstrable value in reporting how quickly a model learns new tasks, as a function of the number of previously acquired tasks.  Most existing meta-learning work ignores this framing, instead choosing to report the capacity to learn new tasks from a fixed small number of samples. We also believe that measuring forgetting should be an inherent component of making learning-related claims, as if a task is immediately forgotten when the model transitions to a new one, it weakens the notion it truly learned it before. 

We found the perspectives introduced by paralleling machine learning and human learning illuminating. Both the baseline model and all query-modulated models we examined show a power law of practice behavior in the number of examples required to reach criterion, as a function of the number of times the task was trained on. Human learners show robust evidence for such power laws, although usually examined in reaction time to response, rather than the additional practice required to maintain performance \parencite{Newell1980,Donner2015}. We provide an interpretation of some of the learning dynamics of the models as exemplifying the stability-flexibility tradeoff usually investigated in studies of memory, and believe it provides a useful perspective on these results. Our sequential benchmark maintains a coreset for practicing of previously learned tasks, which has a fixed size in every episode. The fixed total size with an increasing number of tasks results in fewer examples per episode for each previous task as the benchmark proceeds. This formulation could be considered analogous to spaced practice, in which as materials are learned better, they receive fewer repetitions \parencite{Spitzer1939,Greene2008}. From this perspective, allocating an equal number of examples to each previous task may be unideal, compared to assigning more practice to more recently-introduced tasks. To expand in this direction, we could investigate different coreset allocation methods, drawing more on the science of learning literature, and examine how these changes impact performance on the benchmarks. Ideas from cognitive psychology have been recently employed to help understand how neural networks function, with neural networks able to recover similar behavior to humans in a shape-bias test \parencite{Ritter2017} and in an experiment examining closure, recognizing objects as more than the sum of their parts \parencite{Kim2019}.
