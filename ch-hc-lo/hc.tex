\section{Habits of Mind and Foundational Concepts}

\subsection{\#critique}
This work is predicated on a critique of the meta-learning literature. In this section, and in the literature review which comes next, we reviewed a range of works in the meta-learning subfield of machine learning, examining both the proposed methods as well as how they are evaluated. We discovered that the meta-learning attention pays little to no attention to catastrophic forgetting, which motivated the inclusion of literature from the continual/lifelong learning subfield, who do show some concern for forgetting. To provide additional context, we also reviewed some of the older connectionist modeling work from the late 1980s and 1990s. 

Beyond catastrophic forgetting, we discovered that the existing benchmarks in the field do not evaluate the scaling properties of these learning algorithms: how much additional training does a model require to learn a new task, as a function of how many tasks it already knows? How difficult is it for these models to relearn a task, after a later task has been introduced, as a function of how many times the task was previously learned? This critique helped us form the gap analysis which motivates this work:

\subsection{\#gapanalysis}
Two gaps we identified in the meta-learning literature motivate this work. The first is that the meta-learning literature pays little to no attention to catastrophic forgetting. We believe that claims made about how an algorithm meta-learns, or learns how to learn, are only meaningful in the context of retained learning. If we consider human learning to be the hallmark we should strive for (if not surpass), then holding ourselves to a standard of learning quickly and then forgetting is doing us a disservice. In other words, if you cannot show that you retain something you learned, did you truly learn it at all? 

We describe in the end of our Meta-Learning section how few-shot learning inherently introduces some degree of interference, and does not trivially allow to examine how learning scales (and the existing literature ignores this concern). This is not to say that few-shot learning is a useless task. It certainly measures well the capacity of a network to learn how to perform such tasks from minimal information, and with high volatility between each epoch. However, while these models learn fast, they are also likely to forget fast, which this current benchmark does not attempt to quantify. 

Additionally, these tasks do not help us evaluate how learning scales as a function of training. If we begin from the human learner as an ideal, as a person learns how to learn tasks in a particular domain, we expect to see them increasing in efficiency, requiring fewer training examples to pick up tasks as they learn additional tasks. Likewise, we hope that if we train a machine learning algorithm on tasks from the same domain, with sufficient shared structure, the algorithm will improve in its learning efficiency over time. If indeed tasks introduced later require substantially less training to reach an accuracy criterion than earlier ones this would make for a compelling demonstration of learning how to learn.

In the Introduction and Meta-Learning sections, we highlight a gap in the existing literature, and will next describe the solution we propose for it. We start from an existing codebase, which gives us a head start in creating a dataset, and we then demonstrate what sort of adaptations we made and why they were necessary to investigate these properties. We then describe the novel benchmarks we propose to utilize this dataset with, that allow explicitly examining both the scaling behavior of learning algorithms and measuring the degree of catastrophic forgetting they are liable to. 

\subsection{\#hypothesisdevelopment}
The gap analysis presented above motivates the investigation in this work. The dataset, task, and benchmark descriptions below offer our experimental design for how to test these hypotheses. But what, precisely is the motivating hypothesis? 

While we do compare two model architectures, a baseline and a query-modulated processing model, our primary hypothesis is not driven by a belief that one would do better than the other on this task and benchmark (even though we do harbor that belief). Instead, our primary hypothesis is it is worthwhile and meaningful to measure how meta-learning algorithms scale in their training, and how much capable are these models to avoid (or mitigate) catastrophic interference. Because this concern is currently largely ignored, we believe that measuring this capacity on a fairly naive baseline model, without any explicit meta-learning mechanisms, will present meaningful results by itself. It may be the case that this model shows very poor scaling of its ability to learn, requiring similar amounts of training for each additional task, or it could also be the case that this model does show some improved scaling with each additional task from the same domain. Either way, we will be developing a dataset and the methodology to allow exploring these questions, and a baseline set of results upon which other models can attempt to improve. 

It is not the case that we have no prior beliefs on what the results of our experiments will be. For instance, we do believe that even the simplest model will show some improved scaling as a function of the number of tasks and that query-modulated processing will improve upon the baseline results. Rather, our primary and motivating hypothesis is that measuring this scaling behavior and degree of interference will offer a valuable benchmark to quantify the progress of meta-learning research and guide toward more human-like learning. In this sense, it is predicated both on the work on human learning, which shows humans scale learning positively and avoid interference on the level neural network models show, and on the lack of care to these concerns in the existing meta-learning literature. 

On a more meta, research-community level, this work is also predicated on the hypothesis that we are not the only ones who view this consideration as important. If this hypothesis is true, it makes the prediction that we will be able to eventually publish this work, once we complete a few additional steps and write a manuscript to submit to a conference or journal. We firmly believe in this line of work, and this hypothesis creates a very testable prediction, which we hope to be able to report back on in a few months once we further this line of inquiry and reach a publishable point.   

We invoke \#hypothesisdevelopment again in the context of our discussion. The results we observed also motivate future investigations, which we detailed at the end of the discussion. Perhaps the framing we’re most excited about investigating further the stability-flexibility one, in which the model initially learns stable representations, which show lower catastrophic interference but also take longer to learn. As we proceed in the benchmark, and the model learns additional tasks, it changes its representations to more flexible ones, which allow learning new tasks faster, but also show higher degrees of catastrophic interference. Our results so far support this perspective, but we have not yet investigated the nature of the learned representations directly.

If this perspective has merit, the hypothesis is that we would find a way to measure and visualize the representations themselves in a manner conducive to exploring these changes. As the discussion mentions, there is substantial current research on how to visualize the filters and representations learned by convolutional neural networks, but none (that we could find) that examines how these representations change through the course of meta-learning. To begin investigating these hypotheses, we would start by further reviewing the literature on convolutional representation analysis and visualization, to verify there is indeed a gap in the literature. Should such a gap exist, we would then start devising preliminary experiments to run and analyses to conduct.  

\subsection{\#sourcequality}
Beyond strongly situating our analysis of meta-learning and what the field measures or ignores in the literature in the previous section, we took care to perform an additional, thorough literature review. While not encyclopedic, this review encompasses a variety of approaches to meta-learning to situate this work in its context in the field better. Beyond that, we explore influences from visual question answering (which inspired our dataset), human cognition (which helped us design our comparison models), and the lifelong/continual learning literature (which does concern itself with catastrophic forgetting more than meta-learning does).

For this technical glossary, we also sourced references for almost every term. Unlike the main body of the work, which relies on peer-reviewed publications, we understood that a different type of sources fit the purpose of the glossary better. Academic writing is not always optimized for clarity, and peer-reviewed publications are usually aimed at domain experts, allowing the writers to avoid elaborating on most concepts. We, therefore, opted to go for a collection of blog posts, Medium articles, tutorials, and course notes, all of which assume substantially less knowledge and provide more lengthy and understandable explanations. We hope the readers find these sources useful.

\subsection{\#audience}
We aim this work to be understandable to a technically savvy audience with no particular machine learning background, besides having probably heard the term at some point (given it is nigh-impossible to read anything technological and not encounter it. To that end, we wrote an extensive technical glossary, introducing general machine learning terminology, fundamental concepts relevant to neural networks, and particular concepts that are relevant to the dataset and task we created. The hope is that adding these elaborations helps readers who might be familiar with some terms but not others get up to speed on relevant concepts to this work, and improve the ability to understand our technical methods and contributions.

\subsection{\#testability}
Given the description of the hypothesis above, it does not lead to testable predictions of the form “these model would do better than that model under such conditions,” but it does lead to an overall testable form for the work. We make a claim (the hypothesis) that these concerns merit investigation, and can be interrogated well. To test this claim, we need to clear two hurdles. 

The first hurdle is the ability to measure the scaling properties of these different models as a function of learning. The next entry will describe the dataset, task, and benchmarks we develop, which allow to investigating these properties explicitly. We designed an experiment allowing to control for many irrelevant factors, and by that standard, if we can develop an experiment to measure this form of behavior, it is testable. However, our hypothesis was not that this behavior is measurable - we assumed that we could devise a dataset and paradigm that would allow measuring these qualities. 

The second hurdle is the ability of this experiment to generate meaningful results. As evidenced by the fact we are reporting these results, we believe they offer insight into how these (somewhat naive) models scale in their ability to learn as a function of the number of related tasks they already learned. If, for instance, all of the models we evaluate score similarly on the benchmarks, it is either the case that (a) these models are functionally equivalent, in their ability to scale learning and avoid forgetting, or (b) that this benchmark does not evaluate what we had hoped it would evaluate. In other words, the testable prediction is that different models will perform meaningfully differently on this benchmark if we design it well from an experimental perspective (see the next HC note). 

\subsection{\#interventionalstudy}
The construction of the dataset allows us to investigate a number of properties, and the different benchmarks are experimental designs that use this dataset to investigate a particular characteristic we hope to find in meta-learning algorithms. We evaluate a baseline model to establish an expectation and understand what sort of behavior we should expect on this benchmark from a reasonably naive model. We then compare it to a few different variants, which serve as the experimental conditions in this design. The dependent variable we wish to evaluate is the performance on this benchmark, both in how quickly the model learns all ten tasks to criterion and in what the scaling behavior is for the amount of training required for each successive task.

To minimize the effects of extraneous variables, we attempt to control for as many of them as possible. Having noted that the different dimensions (shape, color, and texture) show varying degrees of difficulty, we treat them separately and consider replications over the benchmark in the ten tasks of a single dimension, rather than over all thirty single-feature tasks. To account for order effects, of particular task orders being easier to learn than others (perhaps, hypothetically, it is easier to learn to recognize cubes before spheres than the other way around), we employ a Latin square design, which guarantees that within each block of ten experiments, each task will appear in each ordinal position (first through tenth) exactly once. We perform a few replications of these ten-experiment blocks to increase the robustness of our results. We fix all random seeds, such that the training set is presented to the model in a consistent order for every task replication.

We follow similar principles with the design of the dataset. While we added substantial variation over the original CLEVR dataset in the colors, shapes, and particularly the textures we used, and we randomize the positions of the objects within each image, we decided to hold most other visual factors constant. We removed any jitter in the orientation of the objects, such that other than any minor occlusion, they always appear the same in every image. We similarly eliminated randomization in the location and angle of the camera relative to the objects, and in the location lighting during rendering. If we eventually discover this dataset is too trivial, we can easily reintroduce these factors, but for the time being, removing them allows us to focus on the learning tasks. 

\subsection{\#variables}
The formulation of the benchmarks and experiments is centered around our identification of the relevant variables. In this case, we treat the choice of model as our independent variable, which allows us to compare performance on different models. The direct dependent variables we measure are the accuracy on every query in the benchmark, and how many training examples it takes the model to reach the accuracy criterion for every task. To evaluate the models, we then aggregate these measures by how many times a query was trained on, and how many total queries the model knows, and arrive at plots that allow us to holistically evaluate the scaling behavior and catastrophic forgetting we wish to use to compare models.

We also knew there is a number of extraneous variables we want to control for. We initially separated the results by dimension, to be able to demonstrate that they qualitatively behave similarly, even if some dimensions are harder than others for the model to reason in. We compared the models on identical query orders, to make the comparison valid. We used a Latin square design, which guarantees that over a block of ten queries in a dimension, each query will fall in all ten ordinal positions, mediating order-related effects. We fixed all random seeds, to attempt to remove randomization-related variance. 

We later realized that our choice of introducing queries from a single dimension is itself a variable we can compare on, and so we designed the control (sequential, heterogeneous dimensions) condition to provide an alternative case. When designing that case, we wanted to account for order-related effects as we did before, but this time between the dimensions. To do so, on every control iteration we pick one of the six permutations of the dimensions and sample a query from each dimension until we reach ten queries (to match the sequential, homogeneous dimension condition, which has ten queries in each dimension). This method of selecting the query order makes the control condition maximally different from the regular sequential one, using three to four queries from each dimension, and maximally spacing the queries from each dimension.

In summary, our experimental design took under consideration every variable we identified and could reasonably control for, allowing us to us to focus on comparing different model variants on how well their learning scales and how much catastrophic forgetting they exhibit or comparing the same model in different experimental conditions. In both cases, our independent variable is categorical, while our dependent variables are the quantitative measures we make of the model. To compare, we, therefore, devised plots that explicitly examine the dependent variables for a pair of independent variable settings (see the \#dataviz section below), and performed a statistical sign test on the differences between these models (see the \#significance section below).

\subsection{\#algorithms}
As the annotations above describe, we wrote substantial amounts of code for this project. In many cases, we found that an appropriate choice of algorithm, or especially data structure, can both substantially simplify and accelerate the code. Here are a few examples:

We saved cached results of long computations in a few places, rather than repeating them every time. We did this with the normalization of the dataset, saving the mean and standard deviation in each channel, rather than computing it from the 45,000 image dataset every time we repeated the benchmark (see \texttt{dataset.py:create\_normalized\_datasets}). We similarly saved the processed versions of experiment results to a (separate) cache, rather than querying the Weights \& Biases API and preprocessing anew if we wanted to explore the results in separate notebooks or needed to restart the kernel (see the notebooks whose names end with `\texttt{plot\_experiments.ipynb}' and `\texttt{benchmark\_final\_plots.ipynb}').

We also performed similar caching in our dataset class for the sequential benchmark (\texttt{dataset.py:SequentialBenchmarkMetaLearningDataset}), storing for every query, which images should return true and which ones should return false. We use this data in order to allocate the coreset, which changes every epoch. While we initially first allocated the half assigned to the current task, and only afterward the half assigned to the coreset, we discovered that this order sometimes leaves us with imbalanced data in the coreset, with some previous queries receiving too high a proportion of negative or positive examples. To remedy this, we flipped the order, first allocating examples to the coreset, verifying that each task receives a sufficiently balanced set, and only then assigning the remaining examples to the current task. We reasoned that having a large number of examples assigned to the current task would leave us much less likely to receive an imbalanced subset. 

In this allocation process, we also ran into another issue. At some point, we discovered that allocating tasks to the coreset takes about two minutes, which is longer than what the rest of an iteration through the benchmark (training and testing) takes. We initially had the aforementioned positive and negative images for each task stored in lists, since numpy’s sampling algorithm can sample from lists (and other index-able data types). However, when we then searched for specific examples in these images, we were incurring a linear-time cost, since searching in an unsorted list takes $O(n)$. We realized it would be much wiser to hold these collections on images in sets, which are hash-backed, and therefore allow $O(1)$ searching. The penalty of converting between sets and lists a few times was dwarfed by the benefits from optimizing the search process, and so we repeated this logic in another place in the same function and saw substantial speedups (\texttt{dataset.py:SequentialBenchmarkMetaLearningDataset:start\_epoch}). 

\subsection{\#dataviz}
We created a substantial number of data visualizations to aid in interpreting our results and metrics. Some of our plots are fairly standard, describing the performance of a single model, or group of comparable models, using overall metrics (such as the loss, accuracy, or ROC AUC). Some of our panels depict more complicated behavior, such as the results for each query-modulation level within each query, split by dimension, to allow examining how behavior changes between models and different types of queries. Our most nuanced visualizations present our measures of learning scaling and catastrophic forgetting, as a model accrues additional experience on a given query or the entire domain (a dimension). We also modified these plots to explicitly compare between a pair of models, simplifying comparative evaluation. 

In all plots, we took care to use the appropriate scaling, changing between logarithmic scaling and linear ones based on what made the most sense. We offered axis legends to ease interpretation, as well as color bars to clarify the meanings of the color scales used. Where appropriate and available, we shaded the standard error of the mean around our data, to provide a measure of uncertainty. When a clear reference point exists, such as an accuracy threshold or a baseline difference or ratio, we plotted it as well.

Our results section presents these results, and our discussion section analyzes them. We believe that these data would be harder (if not impossible) to interpret if we tried to condense them to single numbers. We find these plots useful, and they both reveal insights on the data and motivate the discussion and some of the future lines of inquiry suggested.

\subsection{\#induction}
Machine learning models generally, and neural networks specifically, all reason by induction, learning to map patterns and correlations between the inputs and outputs. For this reason, we measure neural networks not by their capacity to perform on the data they observed and trained on, but their ability to generalize to new data, in a test set, on which the model is evaluated but not allowed to learn from. The premise of this work is that when we evaluate meta-learning, the capacity of a model to learn to induce correctly in a new task, we should examine how the learning scales, to see if the model learns to perform inductive judgments faster as it receives additional training. We should also measure catastrophic interference, how much does new knowledge interfere with previously learned inductive judgments. 

We end up identifying that some inductive problems are inherently easier for the models we examined than others -- colors were the simplest, save for two colors which turned out to be perceptually confusing; shapes were also relatively easy, and textures required the most amount of training learn to generalize on correctly. From the control (sequential, heterogeneous dimensions) condition, we learned that our model’s improved scaling is not as much about accrued expertise within a domain, as the control condition showed comparable scaling and weaker interference patterns. We similarly discovered that our alternative, query-modulated model shows slightly better scaling, but most predominantly shows weaker interference patterns. 

It is interesting to note that we do not usually consider interference between old knowledge and new knowledge in humans performing inductive judgments. \textcite{Gopnik} and \textcite{Griffiths2011} investigate the effects of knowledge on causal reasoning in children, but not directly in the context we postulate here. It stands to reason, though, that if a child were learning to identify birds, a simple criterion they might use would be that birds are flying creatures. Once introduced to bats, and told they are not birds, but mammals, the child might confuse other birds as mammals, until developing more exact criteria, that allow recognizing bats (and other flying mammals, such as flying squirrels or flying lemurs\footnote{\url{https://www.quora.com/Why-are-bats-the-only-flying-mammal}}) as mammals. We are not aware of any cognitive or developmental psychology literature examining how human induction judgments change when additional information is presented or new tasks introduced. From a Bayesian perspective, we can consider an abstract hierarchical model, where each category is defined by the parameters of a distribution in a high-dimensional latent space, from which we can sample examples, and as we learn to label these examples, we update our posterior notion of the distribution in the latent space. Learning that not all flying creatures are birds might initially cause too stark a shift in these distributional parameters, which would be eventually remedied as the parameters are refined through additional examples. 

\subsection{\#significance}
For the most part, we provide evidence for our results through visualizations, which we find work better for presenting and comparing the results of these models than statistical tests do. However, to also provide a statistical measure of the differences between the models, we perform sign tests on the results, examining both which models reliably require fewer examples to reach criterion, and which models show lower effects of catastrophic interference. To increase the robustness of the results, we repeat the tests while accounting for the standard errors of the mean (SEM) around each per-model average, such that we only consider differences that are not within their respective SEMs. While not a measure of practical significance per se (as we did not compute a measure of effect size, as those are incompatible with sign tests), it does help points to results that are likelier to carry practical significance, in the sense that if we repeat these experiments, we will have a stronger belief that the SEM-corrected comparisons will replicate than the non-SEM-corrected ones. Additionally, if the results do not have overlapping SEM intervals, they must be farther apart, and therefore likelier to merit practical significance, although, we did compute a measure of effect size, as no such measure exists for this sign test. We can also view this correction as reducing the power of this test to detect an effect, but increasing its robustness. When using the SEM-based correction, we reduce the probability of false positives (type I errors), as we avoid counting some differences that are below the SEMs; but we increase the probability of false negatives (type II errors) by making the bar higher. 

\subsection{\#scienceoflearning}
A recurring motif in our discussion is the use of similar examples from cognitive psychology and the science of learning to help interpret our results and our model’s performance. When we designed the task, we realized we have to include a coreset of examples from previously trained tasks, and in fact, that it is insufficient to merely train on them, but that if we want to guarantee the model retains them, we must also make them part of the accuracy criterion to be met before a new task is introduced. We then realized that this resembles the notion of distributed or spaced practice for human learners, where students are encouraged to continue practicing previously learned materials, increasing the temporal distance between repetitions of previously acquired knowledge as it becomes better cemented. This parallel to spaced practice, of continuing to train and test on previous tasks, proved crucial to the ability of our model to learn the benchmark, as the plots in the benchmark formulation section demonstrate.

Under this interpretation, though, our method of allocating coreset examples is unideal. We currently allocate half of the examples in each training epoch to the current task, and the other half, which we term the coreset, to all previously learned tasks. The examples in the coreset are allocated evenly between all previous tasks. To see why this may be unideal, consider the training of the tenth (and last) task in an iteration of our sequential benchmark. Each of the previous nine tasks receives $22,500 / 9 = 2500$ training examples, even though the first task is being learned for the tenth time, while the ninth task is only learned for the second time. Since we train on every task in every epoch, our analog for the temporal window in spaced practice is the number of examples allocated in each epoch. 

If we wanted to take on a strategy that more closely matches the recommendations of the cognitive psychology literature, we could change the coreset allocation, such that tasks receive training proportionally to how many times they have been relearned. Presumably, the freshest tasks might require the most amount of continued training, which we can also see in the catastrophic interference results - tasks learned for the first time show the strongest interference once the next task is introduced. We could change the coreset allocation from evenly allocating examples, to allocation proportionally to the number of times a task has been retrained. To measure the effectiveness of such a strategy, and compare it to the current one, we could count the number of `wasted' training examples, which we would define as training examples allocated to a task which is currently above 95\% accuracy. The hypothesis would be that a less naive allocation system, which draws on the science of learning, would ‘waste’ fewer examples. Note that these examples still help the model train, as there is still a loss unless the model allocates the full probability mass to the correct answer, but they do not help the model finish the current stage of the benchmark sooner, as the task they are allocated to is already above the accuracy threshold. 

\subsection{\#professionalism}
We went through the trouble of converting our draft from a Google Document to LaTeX, in order to adhere to a professional standard for a thesis and verify our work looks as clean and elegant as possible. We also recreated several visualizations we had in the Weights \& Biases service using matplotlib, to ensure consistent visual style within all of the plots presented, as well as allow us additional control over the sizes of labels and legends. We also make the entirety of our work open source and document it well, to allow other members of the research community to explore and interact with our work, verify our results, and extend it should they choose to. 

\subsection{\#organization}
We follow the standard form for presenting a research paper in this community. We begin with an introduction, which broadly surveys the field and presents the main line of inquiry we pursue. We follow that with a detailed analysis of the meta-learning literature, which helps identify the gap we explore and how existing work interacts with our ideas. To provide additional context, we offer a broader review of the literature, examining various approaches and better situating our work. We then outline the work we did, starting with the generation of our dataset and formulation of our benchmarks, and following that with a thorough description of the models we compared. We describe our results, offering data visualizations and tables where relevant. We conclude with a discussion, which summarizes our contributions, offers our interpretation of the main finding, and concludes with our plans for future work in this topic. 

\subsection{\#evidencebased}
We attempted to be rigorous and thorough with our use of evidence across this work. To make the case that the existing literature neglects our concerns, of scaling behavior and catastrophic interference, we reviewed it extensively, eventually delving into a second field (lifelong/continual learning) in order to provide additional context on those concerns. To elaborate on the importance of examining these properties, we draw on human learning, which serves as a standard for what learners might be able to do, and we discuss why scaling positively is an important concern for a true meta-learning model, an algorithm that learns how to learn. In the results, we try to back our claims with multiple perspectives on the same evidence, plotting our key figures aligned two separate ways, both by the number of times a task was learned and by the number of tasks a model knows, in order to further illuminate our results. In our discussion, we offer the perspective of examining this model through the stability-flexibility tradeoff often discussed in biological memory, and provide a number of arguments for what that perspective is reasonable to take and well-supported by the results, and useful, in the sense it offers a meaningful interpretation of the results and guides toward future work. 
