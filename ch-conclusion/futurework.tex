\section{Future Work}
We see several lines of work emanating from these results. We hope to take existing meta-learning and lifelong learning models and examine their performance on this benchmark, to measure the effectiveness of specialized architectures, compared to our baseline and query-modulated models. We are curious to see if any existing approaches confer benefits to both the scaling behavior and the degree of catastrophic forgetting, or if, as we might expect, approaches from the meta-learning literature aid in learning faster, while approaches from the lifelong learning field reduce the amount of catastrophic forgetting observed. We also plan to implement the second benchmark we described, which will allow us to investigate the scaling behavior of learning and the degree of forgetting exhibited in a compositional setting, which better mimics the demands of learning to reason in the real world. When we do that, we will use the lessons learned during the execution of the sequential benchmark. We are considering using a validation set to govern the transition between the phases of the benchmark, rather than determining when to introduce the next task based on the performance on the test set. We might also change one of the perceptually challenging colors, to remove that source of noise in the final results. If we modify the sequential benchmark, we might also investigate different coreset allocation schedules, drawing on the distributed practice literature in cognitive psychology.  

In the discussion, we outlined several interesting analyses we could perform by analyzing the representations learned by these models as they progress in the sequential benchmark. We posit several hypotheses for how the different models we benchmarked navigate the stability-flexibility tradeoff, and how representations might change over the course of training. While there is substantial literature on visualizing convolutional features through training \parencite{Zeiler2014,Olah2017}, we are unaware of any work visualizing features and representations in a meta-learning setting, and examining how representations shift as the model learns additional tasks. We could also attempt to visualize the effects of query-based modulation, which would allow to reason about which convolutional features contribute to the identification of which features. As with the dataset and benchmarks, which we will make available to the research community, we hope that developing novel ways to examine representations during the course of meta-learning will elucidate the behavior of these algorithms and drive research in this realm forward.

We also noted that our formulation of task-based processing is more straightforward than existing approaches, such as FiLM 
\parencite{Perez2017,Dumoulin2018}. Our results demonstrated that modulation at the later convolutional stages provided better results than earlier ones, but that may be conflated with the fact that introducing query-modulation at a later layer allows learning more parameters. This growth in the number of parameters is because our architecture includes more filters in each successive convolutional layer, which corresponds to the size of the query-modulation layer. We could perform ablation studies using models with equal numbers of filters in every convolutional layer group, and see if the effect holds. We are also considering investigating our simple query-based modulation in more complicated settings, such as CLEVR, or one of the visual question answering datasets. 
 