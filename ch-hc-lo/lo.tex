\section{Learning Outcomes}

\subsection{Capstone \#metalearning}
Components of the application of this learning outcome for the capstone project stem from the HCs detailed for this section and the next several ones. We believe that our analysis of the field of meta-learning was sound, and allowed us to offer a novel perspective on the problem of meta-learning, by suggesting to examine both the scaling of learning and the prevalence of catastrophic interference. The former of these is effectively ignored in the meta-learning literature, while the latter is mentioned in passing. We developed metrics and plots that allow examining and visualizing both, which we will showcase in the results section. In this section, we design a dataset and task setting and propose two benchmarks that allow empirically exploring these concerns. The next sections of this paper will explore the models we compared, the results we collected, and a discussion of the implications of our findings and relevant next steps to explore.

\subsection{CS156 \#classification}
We compared the task paradigm we proposed to several other supervised learning, computer vision paradigms, and explicitly drew out the differences in order to explain why we consider it a meta-learning problem, and how it differs from existing tasks investigated in the field. The rubric for this LO offers the possibility of “analyzing, explaining, or justifying the application in a way appropriate to the given context.” The description in this setting serves as the analysis and justification of why we treat our problem as a supervised classification problem and employ similar methods, explicitly noting it can be viewed as a logistic regression problem from [image space] X [query space] to a binary label. The models compared (below) offer an implementation of neural network models for this precise setting. Note that both the activation function we used on the last layer of the models (the log-softmax function) and the associated loss function (negative log-likelihood, related to cross-entropy) are standard choices in classification paradigms, further reinforcing the connection to classification problems. Additionally, the standard error metrics we will report, accuracy and ROC AUC (Receiver Operating Characteristic Area Under the Curve) are standard choices in classification settings.

\subsection{Capstone \#datasetsandbenchmarks}
We developed a novel dataset tailored to allow investigating the sort of questions we would like to ask. We started from the CLEVR dataset and their open-sourced generation code and modified it heavily for our purposes. As detailed in the section, we added colors, object shapes, and a variety of textures (materials), allowing us to have ten unique queries in each dimension, creating symmetry between them. We then reduced other sources of visual variance, such as jitter in the location of the camera and orientation of the object, to simplify some components of the visual challenge. We noted that if it proves overly trivial, we can introduce them later, but reasoned that removing these types of noise will help us focus on the questions we want to ask.

We then suggested two novel benchmarks with this dataset, implementing one in this work and leaving another for future work. These benchmarks employ the characteristics introduced in the dataset, allowing us to examine the performance when all tasks introduced are in the same dimension (homogeneous dimension), and later, as a control, when the task introduced vary between the dimension (heterogeneous dimensions). Introducing the tasks sequentially allows us to examine how learning scales as a factor of two variables: first, by how many times the model was retrained on a particular task, and second, by the order in which the tasks were introduced, which corresponds to how many tasks the model was training on at that point in time. We similarly examine how catastrophic forgetting varies by these two variables. Effectively, the first measure allows us to quantify if what is helpful is practice on a given task, and the second measure if what is helpful is experience with a given domain (in this case, a particular dimension). We will report in the results plots allowing to examine both of these types of scaling, both within a single model, and comparing a pair of models, creating a standard to which we (and other members of the research community) can compare additional models to in the future.

\subsection{CS156 \#neuralnetworks}
We picked a neural network design appropriate to the context and task it is solving, similar in design to models used in the literature. We justified both its use and the modifications made by introducing query-modulated visual processing to this task. We believe our baseline models makes for a reasonable standard for comparison, and our query-modulated design is the minimally-complex sophisticated design that might show better performance and operationalize the ideas we had in mind. We implemented these models in PyTorch and compared them extensively in our results. We believe our review of the literature should serve as sufficient evidence that there is no simpler model we could have tested and expected to perform equally well on this task. 

\subsection{Capstone \#softwareimplementation}
We implemented the models, dataset generation, benchmarks, and analysis and plotting code in Python. While we developed different parts of the codebase to slightly different standards, we attempted to follow best practices for research code in all code written. Variable names were chosen to be consistent and clear. Code reuse was encouraged by splitting functions at appropriate boundaries, and using inheritance in most places it was sensible. Most functions receive optional arguments to flexibly allow for multiple behaviors with the same underlying logic. Each folder offers a readme, and most functions are documented, offering a description of the purpose of the function as well as the essential inputs and the format of outputs. 

The code is still research-level code, rather than production-oriented, so it is imperfect at times. We did not implement any unit tests, although we manually tested the behavior of most of the code. There exists some code duplication in cases in which it was easier and faster to duplicate some logic rather than abstract it out; this is a clear minority of the code written, but it still exists. Overall, we are proud of the code we wrote for this project, and are happy to have it representing us on GitHub. 

The main package of research code is here: \url{https://github.com/guydav/deep-learning-projects/tree/master/projects/metalearning}
The analysis and visualization code and notebooks using them are here: \url{https://github.com/guydav/deep-learning-projects/tree/master/notebooks}
The dataset generation code, which is less polished, as it is forked from the original CLEVR-dataset-gen repository (which is not very elegantly written), is here: \url{https://github.com/guydav/clevr-dataset-gen}

\subsection{CS156 \#overfitting}
We demonstrated that overfitting occurred on our small dataset, and then that it still occurred on the full dataset, albeit to a much lower degree than with limited data. Afterward, we evaluated two standard measures to overcome overfitting from the literature, and once discovered that one substantially outperforms the other, used it to perform all remaining experiments. In a sense, increasing the size of the dataset is also an overfitting prevention measure, as adding additional data allows the model to discern the patterns that generalize to the test set much more easily than without it. However, even with the added data, we discovered that adding weight decay (which corresponds to a Gaussian prior on the weights) still helped to improve performance, both on the loss, and more importantly, on the AUC. 

\subsection{CS156 \#modelmetrics}
First, we evaluated the baseline model on the entire dataset, in what we called the simultaneous training, heterogeneous dimensions condition. We initially reported the loss and accuracy and then transitioned to the AUC ROC, which we deemed as more appropriate, as it evaluates the model over a range of decision thresholds, and spares us from attempting different thresholds ourselves. Then, we developed unique performance metrics to allow evaluating both the scaling of learning and catastrophic forgetting in the context of the benchmark. We did not try to summarize these entire metrics as single numbers, but rather opted for data visualizations, which capture the performance on our benchmarks. We developed visualizations both for the metrics on a single model, as well as for comparing two models on these metrics, performing the appropriate comparison function (division or subtraction) based on the details of the metric. While we did not contract the performance to a single metric number, we did also implement a sign test to statistically compare between the performances of competing models using these metrics.
