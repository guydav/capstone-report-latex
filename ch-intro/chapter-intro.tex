\chapter{Introduction\label{ch:intro}}

Machine learning (particularly deep learning) is amidst a transformational upheaval of society’s ability to make sense from large amounts of data (see, among many others, \cite{Sejnowski2018}). These results span across many domains in disciplines. Computer vision showed the first seeds of the power of deep learning with \possessivecite{Krizhevsky2012} AlexNet, which shattered the state of the art results on the ImageNet \parencite{Deng2009} and ignited a flurry of renewed research in the field. Academic progress demonstrated a rapid transition to these methods: convolutional neural networks, which underlie \possessivecite{Krizhevsky2012} results, are used in Google’s search engine to classify images (described by \cite{Marr2017}), Facebook’s face recognition capabilities \parencite{Taigman2014}, Nvidia’s work on self-driving cars \parencite{Bojarski2016}, among many others. Computer vision is far from the only problem area showing such drastic transformations. In the field of natural language processing, deep neural networks power developments in machine translation \parencite{Wu2016a}, question answering, and semantic analysis \parencite{Devlin2018a}. Perhaps most famously, DeepMind’s AlphaZero \parencite{Silver2018} established itself as the strongest chess and Go player alive, beating all contenders, human and artificial alike.  

Beneath the surface of this computational revolution lies a worrying reliance on increasingly large amounts of labeled data and compute time. In supervised learning problems, which comprise the majority of advances in computer vision and natural language processing, current approaches require gargantuan amounts of labeled data in order to reach peak results. \possessivecite{Deng2009} ImageNet included approximately three million images. Since then, Google released the nine-million image Open Images dataset \parencite{Kuznetsova2018} and eight-million, over 500,000 hours of video YouTube-8M dataset \parencite{Abu-El-Haija2016}, and Facebook's \textcite{Mahajan2018} describe training on billions of images. Natural language shows similar effects, as \textcite{Devlin2018} describes in his slides both the benefits of increasingly large models and data, particularly the overall reliance on labeled data and the race to learn to do more with weakly labeled (or unlabeled) data: ```Data I can get a lot of without paying anyone' vs. `Data I have to pay people to create'.'' Reinforcement learning (RL) problems, such as the AlphaZero, do not require data but require almost-absurdly large amounts of playing time, requiring forty-four million games to reach superhuman performance in chess, and over one-hundred and forty million games for Go. \textcite{Lake2017} demonstrate the discrepancy between state of the art RL approaches on Atari games and human performance, where non-expert humans take fifteen minutes to reach performance levels that take the algorithms hundreds of hours to attain. 
 
Meta-learning, or learning to learn, is a branch of machine learning research exploring one potential avenue to learn more efficiently and reduce the reliance on massive datasets. In a standard machine learning setting, we attempt to devise an algorithm that can learn how to solve a task from data. In meta-learning, we consider a problem one level of abstraction above and devise algorithms that can learn to solve a series or family of tasks. If these tasks share some underlying structure, we hope a robust meta-learning algorithm to be able to exploit it to learn a group of tasks more efficiently. This both assumes such a shared structure exists, and that the differences between tasks do not overshadow its usefulness. Informally, if the tasks are more alike than they are different, we hope meta-learning to be able to do well, by sharing learning between the tasks. As \textcite{Hochreiter2001} among others point out, meta-learning has a natural interpretation in a Bayesian setting, as a hierarchical model where the hyperprior informs the prior for each task, and the inference for a posterior for a given task also allows updating the hyperprior itself for future tasks. \textcite{Lansdell2018} offer a slightly more philosophical perspective: in early artificial intelligence research, human researchers designed algorithms for particular problems. Machine learning offered the insight of devising methods of learning the solutions from data, rather than crafting them from the ground up. However, if we believe in learning a solution from data, why not attempt to learn the learning rule as well, or allow the data to inform and modify it? 

Meta-learning research spans a history of over thirty years, going as far back as \possessivecite{Schmidhuber1987} thesis and\possessivecite{Bengio1991} work on learning synaptic rules. Another notable early work comes from \textcite{Hochreiter2001}, who devised a recurrent neural network-based (RNN) formulation of the problem, capable of meta-learning using standard gradient-based approaches. The recent rise of deep learning triggered a newfound interest in meta-learning, reviewed by \textcite{Lansdell2018}. While some recent meta-learning research attempts to tackle meta-reinforcement learning, the majority of the field focuses on supervised learning problems, primarily classification. The task and benchmarks formulated in this work as a simplified form of visual question answering (VQA; \cite{Antol2015}), which most resembles classification problems, so those serve as a basis for comparison. The default evaluation used for classification meta-learning algorithms is few-shot learning, specifically few-shot classification, which goes back to \textcite{LiFei-Fei2006}. In this setting, a model is trained on a few labeled items from several new (to the model) classes and is tested on classifying unlabeled examples into one of these new classes. Few-shot classification is often tested on the Omniglot dataset (\cite{Lake2015}; \cite{Lake2019}) or the mini-ImageNet setting introduced by \textcite{Vinyals2016}. This task embodies a particular type of meta-learning humans excel at, of rapidly processing inputs and categorizing them (\cite{Thorpe1996}; \cite{FeiFei2002}).

In this work, we argue for another important desideratum for meta-learning algorithms. Inspiration comes from \textcite{Thrun1996}, who asked “is learning the n-th thing easier than learning the first?” and from humans, who, of course show incredible capacity to learn and acquire skills in a variety of domains and contexts (\cite{Brown1988}; \cite{Green2008}; \cite{Krakauer2011}). We believe the capacity to continue learning additional tasks, without forgetting previous ones, should be a core requirement of good meta-learning algorithms; what good is the ability to efficiently learn new tasks, if the model then forgets the previous ones? While the meta-learning literature tends to neglect this concern, \textcite{Parisi2019} review the lifelong/continual learning \parencite{Thrun1995} literature in neural networks, and discuss several measures of evaluating lifelong learning models for in the presence of multiple tasks. In that setting, a standard benchmark is incremental learning \parencite{Kemker2017a}, examining how accurate a model remains as additional classes are introduced. We present a dataset and testing paradigm that, beyond allowing to measure the interference between learned tasks, also enables evaluating the scaling behavior of the model (in the amount of training required to learn a new task) as a function of the number of tasks previously acquired.  

The remainder of the work is structured as follows: first, we review the formalisms of meta-learning, continual learning, and transfer learning, to provide a better grounding for the setting of this work. We then review the array of approaches taken in the meta-learning literature, particularly noting how authors tend to evaluate and compare their models. We describe the creation of our new dataset and task paradigm, and detail two benchmarks enabled by our dataset to investigate the scaling capacity of meta-learning algorithms. We provide the results of a few pilot experiments using this dataset and provide a discussion of our results so far, as well as future work we hope to do before submitting a manuscript on this work to peer-reviewed venues. 

