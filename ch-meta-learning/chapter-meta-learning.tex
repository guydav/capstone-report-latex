\chapter{Meta-Learning\label{ch:meta-learning}}

In this section, we provide a more extended and more formal introduction to meta-learning and related problem formulations, to better ground the remainder of this work and make the argument for scaling behavior is an essential consideration in the evaluation of meta-learning algorithms. Due to the focus of this work, we will focus on the formal description of meta-learning a supervised classification problem, but we may note that with slight modifications, these formalisms can be adapted to regression problems (\cite{Finn2017}; \cite{Garg2016}), reinforcement learning (\cite{Wang2016}; \cite{Duan2017}), and unsupervised learning (\cite{Garg2018}; \cite{Hsu2019}).

In a standard classification problem, a model attempts to learn a single task $T$, which requires learning to map inputs $X$ to class labels $y$, learning a model for the conditional distribution $P(y | X)$. A baseline case for such problems is logistic regression: given a single class, of which different training examples may or may not be a member of, a model learns to predict a probability of membership in the class. Standard classification problem assume that the distribution of training examples is stationary, and reflects the distribution of unobserved test examples well; if these distributions are markedly different, or additional classes or examples are later introduced, classifiers tend to fail in unexpected ways (\cite{Amodei2016}; \cite{Hu2018}). 

Adapting the formalism proposed by \textcite{Finn2017}, in a meta-learning setup, rather than consider training over a single task, we assume a distribution over tasks, $P(T)$. At each episode, we draw a classification task $T_i$ from the overall distribution and train the model on it. Through the course of training, we draw and train on additional tasks, ideally, improving the capacity of the model to learn successive tasks; in other words, we expect the model to improve in its capacity to learn new tasks throughout training. In a standard meta-learning setting, \textcite{Finn2017} point out that additional tasks are usually held out for meta-testing, testing the meta-learning capacity. We will observe later that our benchmarks take a different approach to evaluate whether the ability to learn new tasks improves over time. 

In this formulation, we evaluate meta-learning models only on their capacity to acquire additional tasks, not to retain previous ones. To the extent that we wish machine learning to reflect human learning, this sort of evaluation does not accurately reflect human learning behavior. As early as preschool ages, humans show the ability to quickly learn from instructions and transfer approaches to new problems \parencite{Brown1988a}, and adults show the capacity to learn flexibly and rapidly over a wide array of domains \parencite{Green2008}. Interference in human learning has often been investigated in sensorimotor learning contexts. \textcite{Krakauer2011} describe anterograde interference, the ability previously learned tasks to impede the learning of new ones. Both \textcite{Shadmehr1994} and \textcite{Brashers-Krug1994} provide evidence for such interference in humans in some cases, but not others. In the domain of category learning, \textcite{Waldron2001} demonstrate different sources of interference under different task demands, and \textcite{Mareschal2002} show interference in category acquisition in 3-4 month-old infants. Intuitively, while humans show evidence for varying degrees of interference, across different contexts, induced both by learning new tasks and independently forgetting old ones, it is far from the degree demonstrated by neural network models. 

The problem of catastrophic interference (or catastrophic forgetting) has been investigated in connectionist networks (the early precursor to today's deep neural networks) since the field's inception. \textcite{McCloskey1989} provided early evidence for learning-induced interference and discussed contexts in which it is more liable to occur, and \textcite{Kruschke1993} suggested a learning algorithm for such models, inspired by human category learning, less liable to such interference. \textcite{French1999} reviewed evidence on forgetting in connectionist models, using an introduction quite illuminating to our motivations:
\begin{quote} \onehalfspacing
``All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically'. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting.'' 
\end{quote}

While the meta-learning community has not paid substantial attention to this concern, two other branches of research in machine learning treat it more explicitly. The first is lifelong (or continual) learning \parencite{Thrun1995}. \textcite{Parisi2019} offer a review of this field, which they define as an ability to learn from continually arriving information, which they note tends to lead to interference between new information and previous learning. The second pertinent branch of research comes from transfer learning (\cite{Pratt1993}; \cite{Caruna1997}), which investigates the ability to employ knowledge gained in learning to solve one problem in solving additional problems. Surveying a few recent publications that mention some combination of the terms ``catastrophic,'' ``forgetting,'' and ``interference,'' we find:
\begin{tightemize}
\item \textcite{Santoro2016} propose a model using external memory storage, which they wipe between meta-learning episodes. When they test the model without clearing the memory, they find substantial interference between tasks; however, clearing the memory certainly impedes retaining tasks learned in previous episodes. 

\item \textcite{Al-Shedivat2018} (building on \cite{Finn2017}) mention the term catastrophic forgetting as something to be avoided but do not discuss it further.

\item \textcite{Munkhdalai2018} explore forgetting more explicitly by training their model on one dataset (Omniglot; \cite{Lake2015}), then training on a second (MNIST; \cite{Lecun1998}), and exploring the drop in accuracy on Omniglot as a function of MNIST training. 

\item \textcite{Aljundi2018} fall more into the lifelong learning camp, offering multiple benchmarks both examining accuracy drop on a first task after training on a second, as well as on longer sequences of problems of up to five tasks.

\item \textcite{Lopez-Paz2017} provide another example of lifelong learning, explicitly measuring both backward and forward transfer, which they define as the effect of each learned task on those learned before and after it.
\end{tightemize}

Beyond explicitly measuring and accounting for interference, a second concern the research community seems to ignore is the scaling behavior of such meta-learning algorithms, in term of the number of iterations required to learn each task. None of the works surveyed attempts to explicitly answer \possessivecite{Thrun1996} question, ``is learning the n-th thing easier than learning the first?'' We argue that if a model is truly learning to learn, then it would show increased learning efficiency, and would show a negative correlation between the number of tasks learned and the amount of training required to acquire the next task, ideally showing some degree of asymptotic behavior. We would also like this behavior to occur with limited catastrophic forgetting, retaining a high level of performance on previously acquired tasks while learning the current one. In order to examine this scaling behavior, we need to devise a dataset and collection of tasks that allow investigating the scaling behavior and measuring the amount of training required for each additional task. We next describe how we created such a dataset and two benchmarks using that allow us to analyze precisely this sort of behavior.

We conclude this review by noting that one of the primary benchmarks used to evaluate meta-learning, the few-shot learning paradigm, fails to account for both of these concerns: it inherently introduces some degree of forgetting with the structure of the task and does not allow to examine how learning scales with additional training. In a few-shot learning context, the network is presented with a few classes (usually denoted by $N$) at every training epoch, and a limited number of training examples for each class (usually denoted by $K$), which yields the $N$-way, $K$-shot learning moniker. In this setting, the network does not know how many classes exist in total, only needing to classify between the $N$ current ones at every epoch. Network architectures tend to operationalize this design by having $N$ output units, each the probability of being a member of one of these $N$ classes. For instance, for $N=3$ on the mini-ImageNet \parencite{Vinyals2016} setting, in one epoch the classes might be ‘scorpion,' ‘Pomeranian,' and ‘car wheel,' mapped to the first, second, and third output unit respectively. In the next epoch, they might be ‘dumbbell,' ‘mixing bowl,' and ‘stove,' mapped to the same three output units. Understandably, if a network is first tasked at mapping Pomeranians to a particular output unit, and then at mapping mixing bowls to the same unit, it might forget about the Pomeranians, which have absolutely nothing in common with the mixing bowls. Alternatively, perhaps it would retain some of the visual processing elements useful in detecting what a Pomeranian is, but lose the mapping of those features to the relevant output unit. Either way, the model is unlikely to maintain the ability to map both Pomeranians and mixing bowls to the same output. The use of a fixed number of examples also does not allow to trivially evaluate how well the acquisition of each task becomes easier with learning, since the training is not to a fixed accuracy criterion, but rather to a fixed amount of data. We could imagine evaluating the capacity to perform $N$-way, $K$-shot learning as a function of the amount of training received, but the literature reviewed does not do that either - instead opting to only report the final performance at the culmination of training. 
